{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a227374c",
   "metadata": {},
   "source": [
    "Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8dd09d9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmentation\n",
      "Brand_Image\n",
      "Brand_Health\n",
      "Companion\n",
      "Competitor\n",
      "Dayofweek\n",
      "Daypart\n",
      "Needstate\n",
      "SA_var\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. Segmentation\n",
    "print(\"Segmentation\")\n",
    "df_seg = pd.read_csv('Data/2017Segmentation3685case.csv', sep=';', on_bad_lines='skip')\n",
    "df_seg.columns = df_seg.columns.str.strip()\n",
    "\n",
    "# 2. Brand_Image\n",
    "print(\"Brand_Image\")\n",
    "df_image = pd.read_csv('Data/Brand_Image.csv', sep=';', on_bad_lines='skip')\n",
    "df_image.columns = df_image.columns.str.strip()\n",
    "\n",
    "# 3. Brand_Health\n",
    "print(\"Brand_Health\")\n",
    "df_health = pd.read_csv(\n",
    "    'Data/Brandhealth.csv',\n",
    "    sep=';',\n",
    "    on_bad_lines='skip',\n",
    "    low_memory=False\n",
    ")\n",
    "df_health.columns = df_health.columns.str.strip()\n",
    "\n",
    "# 4. Companion\n",
    "print(\"Companion\")\n",
    "df_companion = pd.read_csv('Data/Companion.csv', sep=';', on_bad_lines='skip')\n",
    "df_companion.columns = df_companion.columns.str.strip()\n",
    "\n",
    "# 5. Competitor\n",
    "print(\"Competitor\")\n",
    "df_competitor = pd.read_csv('Data/Competitor database_xlnm#_FilterDatabase.csv', sep=';', on_bad_lines='skip')\n",
    "df_competitor.columns = df_competitor.columns.str.strip()\n",
    "\n",
    "# 6. Dayofweek\n",
    "print(\"Dayofweek\")\n",
    "df_dow = pd.read_csv('Data/Dayofweek.csv', sep=';', on_bad_lines='skip')\n",
    "df_dow.columns = df_dow.columns.str.strip()\n",
    "\n",
    "# 7. Daypart\n",
    "print(\"Daypart\")\n",
    "df_daypart = pd.read_csv('Data/Daypart.csv', sep=';', on_bad_lines='skip')\n",
    "df_daypart.columns = df_daypart.columns.str.strip()\n",
    "\n",
    "# 8. Needstate\n",
    "print(\"Needstate\")\n",
    "df_need = pd.read_csv('Data/NeedstateDayDaypart.csv', sep=';', on_bad_lines='skip')\n",
    "df_need.columns = df_need.columns.str.strip()\n",
    "\n",
    "# 9. SA_var\n",
    "print(\"SA_var\")\n",
    "df_sa = pd.read_csv('Data/SA#var.csv', sep=';', on_bad_lines='skip')\n",
    "df_sa.columns = df_sa.columns.str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4017456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Brand_Image is consistent with SA#var\n",
      "âœ… Brand_Health is consistent with SA#var\n",
      "âœ… Companion is consistent with SA#var\n",
      "âœ… Dayofweek is consistent with SA#var\n",
      "âœ… Daypart is consistent with SA#var\n",
      "âœ… NeedstateDayDaypart is consistent with SA#var\n"
     ]
    }
   ],
   "source": [
    "# ğŸ” Táº¡o dict mapping ID -> (City, Year) tá»« df_sa\n",
    "id_map = df_sa.set_index('ID')[['City', 'Year']].dropna().astype(str)\n",
    "\n",
    "# ğŸ§  HÃ m kiá»ƒm tra tá»«ng dataframe\n",
    "def check_consistency(df, name):\n",
    "    df = df[['ID', 'City', 'Year']].dropna().astype(str)\n",
    "    df = df[df['ID'].isin(id_map.index)]\n",
    "    merged = df.merge(id_map, on='ID', suffixes=('', '_sa'))\n",
    "    mismatch = merged[(merged['City'] != merged['City_sa']) | (merged['Year'] != merged['Year_sa'])]\n",
    "    \n",
    "    if not mismatch.empty:\n",
    "        print(f\"âŒ Inconsistent City/Year found in {name}: {len(mismatch)} mismatches\")\n",
    "        display(mismatch.head())\n",
    "    else:\n",
    "        print(f\"âœ… {name} is consistent with SA#var\")\n",
    "\n",
    "# ğŸ“‹ Danh sÃ¡ch sheet cáº§n kiá»ƒm\n",
    "sheets = {\n",
    "    'Brand_Image': df_image,\n",
    "    'Brand_Health': df_health,\n",
    "    'Companion': df_companion,\n",
    "    'Dayofweek': df_dow,\n",
    "    'Daypart': df_daypart,\n",
    "    'NeedstateDayDaypart': df_need,\n",
    "}\n",
    "\n",
    "# ğŸ” Cháº¡y kiá»ƒm tra\n",
    "for name, df in sheets.items():\n",
    "    check_consistency(df, name)\n",
    "def clean_brand_columns(df, columns, replace_map, name=\"DataFrame\"):\n",
    "    \"\"\"\n",
    "    Chuáº©n hoÃ¡ cÃ¡c cá»™t thÆ°Æ¡ng hiá»‡u trong DataFrame:\n",
    "    - Viáº¿t thÆ°á»ng\n",
    "    - XoÃ¡ khoáº£ng tráº¯ng\n",
    "    - Thay tháº¿ cÃ¡c biáº¿n thá»ƒ thÆ°Æ¡ng hiá»‡u theo báº£n Ä‘á»“ thay tháº¿\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame cáº§n xá»­ lÃ½\n",
    "        columns (list): Danh sÃ¡ch tÃªn cá»™t cáº§n xá»­ lÃ½\n",
    "        replace_map (dict): Tá»« Ä‘iá»ƒn Ã¡nh xáº¡ giÃ¡ trá»‹ cáº§n thay tháº¿\n",
    "        name (str): TÃªn báº£ng Ä‘á»ƒ in log\n",
    "    \"\"\"\n",
    "    for col in columns:\n",
    "        if col not in df.columns:\n",
    "            print(f\"âš ï¸  Cá»™t '{col}' khÃ´ng tá»“n táº¡i trong {name}\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\nğŸ” Unique values in '{col}' BEFORE cleaning ({name}):\")\n",
    "        print(df[col].dropna().unique())\n",
    "\n",
    "        df[col] = df[col].str.lower().str.strip()\n",
    "        df[col] = df[col].replace(replace_map)\n",
    "\n",
    "        print(f\"âœ… Unique values in '{col}' AFTER cleaning ({name}):\")\n",
    "        print(df[col].dropna().unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b21eaa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“„ Äang xá»­ lÃ½: Segmentation\n",
      "ğŸ” Sá»‘ dÃ²ng trÃ¹ng láº·p: 0\n",
      "ğŸ’¾ ÄÃ£ lÆ°u: Segmentation_cleaned.csv\n",
      "\n",
      "ğŸ“„ Äang xá»­ lÃ½: Brand_Image\n",
      "ğŸ” Sá»‘ dÃ²ng trÃ¹ng láº·p: 13974\n",
      "âœ… ÄÃ£ loáº¡i bá» 13974 dÃ²ng trÃ¹ng láº·p\n",
      "ğŸ—‘ ÄÃ£ loáº¡i cá»™t: City\n",
      "ğŸ—‘ ÄÃ£ loáº¡i cá»™t: Year\n",
      "ğŸ”§ ÄÃ£ chuáº©n hoÃ¡ cá»™t Attribute\n",
      "ğŸ”§ ÄÃ£ chuáº©n hoÃ¡ cá»™t Awareness\n",
      "ğŸ”§ ÄÃ£ chuáº©n hoÃ¡ cá»™t BrandImage\n",
      "ğŸ’¾ ÄÃ£ lÆ°u: Brand_Image_cleaned.csv\n",
      "\n",
      "ğŸ“„ Äang xá»­ lÃ½: Brand_Health\n",
      "ğŸ” Sá»‘ dÃ²ng trÃ¹ng láº·p: 0\n",
      "ğŸ—‘ ÄÃ£ loáº¡i cá»™t: City\n",
      "ğŸ—‘ ÄÃ£ loáº¡i cá»™t: Year\n",
      "ğŸ”§ Chuáº©n hoÃ¡ cá»™t Brand trong báº£ng Brand_Health\n",
      "ğŸ”§ Chuáº©n hoÃ¡ cá»™t Awareness trong báº£ng Brand_Health\n",
      "ğŸ”§ Chuáº©n hoÃ¡ cá»™t Spontaneous trong báº£ng Brand_Health\n",
      "ğŸ”§ Chuáº©n hoÃ¡ cá»™t Trial trong báº£ng Brand_Health\n",
      "ğŸ”§ Chuáº©n hoÃ¡ cá»™t P3M trong báº£ng Brand_Health\n",
      "ğŸ”§ Chuáº©n hoÃ¡ cá»™t P1M trong báº£ng Brand_Health\n",
      "ğŸ”§ Chuáº©n hoÃ¡ cá»™t Weekly trong báº£ng Brand_Health\n",
      "ğŸ”§ Chuáº©n hoÃ¡ cá»™t Daily trong báº£ng Brand_Health\n",
      "ğŸ’¾ ÄÃ£ lÆ°u: Brand_Health_cleaned.csv\n",
      "\n",
      "ğŸ“„ Äang xá»­ lÃ½: Companion\n",
      "ğŸ” Sá»‘ dÃ²ng trÃ¹ng láº·p: 799\n",
      "âœ… ÄÃ£ loáº¡i bá» 799 dÃ²ng trÃ¹ng láº·p\n",
      "ğŸ—‘ ÄÃ£ loáº¡i cá»™t: City\n",
      "ğŸ—‘ ÄÃ£ loáº¡i cá»™t: Year\n",
      "ğŸ’¾ ÄÃ£ lÆ°u: Companion_cleaned.csv\n",
      "\n",
      "ğŸ“„ Äang xá»­ lÃ½: Competitor\n",
      "ğŸ” Sá»‘ dÃ²ng trÃ¹ng láº·p: 0\n",
      "ğŸ’¾ ÄÃ£ lÆ°u: Competitor_cleaned.csv\n",
      "\n",
      "ğŸ“„ Äang xá»­ lÃ½: Dayofweek\n",
      "ğŸ” Sá»‘ dÃ²ng trÃ¹ng láº·p: 37\n",
      "âœ… ÄÃ£ loáº¡i bá» 37 dÃ²ng trÃ¹ng láº·p\n",
      "ğŸ—‘ ÄÃ£ loáº¡i cá»™t: City\n",
      "ğŸ—‘ ÄÃ£ loáº¡i cá»™t: Year\n",
      "ğŸ’¾ ÄÃ£ lÆ°u: Dayofweek_cleaned.csv\n",
      "\n",
      "ğŸ“„ Äang xá»­ lÃ½: Daypart\n",
      "ğŸ” Sá»‘ dÃ²ng trÃ¹ng láº·p: 0\n",
      "ğŸ—‘ ÄÃ£ loáº¡i cá»™t: City\n",
      "ğŸ—‘ ÄÃ£ loáº¡i cá»™t: Year\n",
      "ğŸ’¾ ÄÃ£ lÆ°u: Daypart_cleaned.csv\n",
      "\n",
      "ğŸ“„ Äang xá»­ lÃ½: NeedstateDayDaypart\n",
      "ğŸ” Sá»‘ dÃ²ng trÃ¹ng láº·p: 72\n",
      "âœ… ÄÃ£ loáº¡i bá» 72 dÃ²ng trÃ¹ng láº·p\n",
      "ğŸ—‘ ÄÃ£ loáº¡i cá»™t: City\n",
      "ğŸ—‘ ÄÃ£ loáº¡i cá»™t: Year\n",
      "ğŸ’¾ ÄÃ£ lÆ°u: NeedstateDayDaypart_cleaned.csv\n",
      "\n",
      "ğŸ“„ Äang xá»­ lÃ½: SA_var\n",
      "ğŸ” Sá»‘ dÃ²ng trÃ¹ng láº·p: 0\n",
      "ğŸ”§ Chuáº©n hoÃ¡ cá»™t TOM trong báº£ng SA_var\n",
      "ğŸ”§ Chuáº©n hoÃ¡ cá»™t BUMO trong báº£ng SA_var\n",
      "ğŸ”§ Chuáº©n hoÃ¡ cá»™t MostFavourite trong báº£ng SA_var\n",
      "ğŸ—‘ ÄÃ£ loáº¡i cÃ¡c cá»™t dÆ° thá»«a Ä‘áº·c biá»‡t\n",
      "ğŸ’¾ ÄÃ£ lÆ°u: SA_var_cleaned.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# ğŸ“‚ Táº¡o thÆ° má»¥c output náº¿u chÆ°a tá»“n táº¡i\n",
    "output_dir = \"./cleaned_data/\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# ğŸ“Œ Map chuáº©n hÃ³a brand Ä‘á»ƒ dÃ¹ng chung\n",
    "brand_replace_map = {\n",
    "    'street / half street coffee (including carts)': 'street',\n",
    "    'Street / Half street coffee (including carts)': 'street',\n",
    "    'other branded cafe chain': 'other',\n",
    "    'other 1': 'other',\n",
    "    'other 2': 'other',\n",
    "    'other 3': 'other',\n",
    "    'indepedent cafe': 'independent cafe'\n",
    "}\n",
    "\n",
    "# ğŸ§¹ HÃ m chuáº©n hoÃ¡ cá»™t brand\n",
    "def clean_brand_columns(df, columns, replace_map, sheet_name):\n",
    "    for col in columns:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].str.lower().str.strip()\n",
    "            df[col] = df[col].replace(replace_map)\n",
    "            print(f\"ğŸ”§ Chuáº©n hoÃ¡ cá»™t {col} trong báº£ng {sheet_name}\")\n",
    "\n",
    "# ğŸ§  HÃ m tÃ¡ch ngÃ y vÃ  thá»i gian trong NeedstateDayDaypart\n",
    "def clean_day_daypart(val):\n",
    "    val = str(val).strip().lower()\n",
    "    if val in ['overall', 'weekdays', 'weekends']:\n",
    "        return pd.Series({'DayGroup': val, 'Daypart_cleaned': None})\n",
    "    else:\n",
    "        return pd.Series({'DayGroup': None, 'Daypart_cleaned': val})\n",
    "\n",
    "# ğŸ§  HÃ m chuáº©n hoÃ¡ needstates\n",
    "def standardize_needstate(val):\n",
    "    val = str(val).strip().lower()\n",
    "    typo_map = {\n",
    "        'socialzing': 'socializing with others',\n",
    "        'enterntainment (watching movies. playing games, browsing web,â€¦)': 'entertainment',\n",
    "        'enterntainment': 'entertainment'\n",
    "    }\n",
    "    val = typo_map.get(val, val)\n",
    "    needstate_map = {\n",
    "        'socializing with colleagues': 'socializing with others',\n",
    "        'socializing with family / relatives': 'socializing with others',\n",
    "        'socializing with friends': 'socializing with others',\n",
    "        'drinking coffee': 'drinking',\n",
    "        'drinking tea': 'drinking',\n",
    "        'drinking ice-blended': 'drinking',\n",
    "        'drinking other beverages (excluding tea, coffee, freeze)': 'drinking',\n",
    "        'entertainment': 'entertainment',\n",
    "        'relaxing (alone)': 'relaxing',\n",
    "        'have meals (breakfast / lunch / dinner)': 'eating',\n",
    "        'have snack / pastry': 'eating',\n",
    "        'working / business meeting': 'working',\n",
    "        'studying / reading books': 'studying'\n",
    "    }\n",
    "    return needstate_map.get(val, val)\n",
    "\n",
    "# ğŸ“¦ HÃ m xá»­ lÃ½ táº¥t cáº£ cÃ¡c báº£ng\n",
    "def process_df(df, name, drop_cols=['City', 'Year']):\n",
    "    print(f\"\\nğŸ“„ Äang xá»­ lÃ½: {name}\")\n",
    "    print(f\"ğŸ” Sá»‘ dÃ²ng trÃ¹ng láº·p: {df.duplicated().sum()}\")\n",
    "\n",
    "    df_clean = df.drop_duplicates()\n",
    "\n",
    "    if df.shape[0] != df_clean.shape[0]:\n",
    "        print(f\"âœ… ÄÃ£ loáº¡i bá» {df.shape[0] - df_clean.shape[0]} dÃ²ng trÃ¹ng láº·p\")\n",
    "\n",
    "    for col in drop_cols:\n",
    "        if col in df_clean.columns:\n",
    "            df_clean = df_clean.drop(columns=col)\n",
    "            print(f\"ğŸ—‘ ÄÃ£ loáº¡i cá»™t: {col}\")\n",
    "\n",
    "    if name == \"SA_var\":\n",
    "        cols_to_drop = ['MPI_Mean_Use','MPI#detail','MPI','Age#group','Col']\n",
    "        df_clean = df_clean.drop(columns=[c for c in cols_to_drop if c in df_clean.columns])\n",
    "        brand_columns = ['TOM','BUMO','MostFavourite']\n",
    "        clean_brand_columns(df_clean, brand_columns, brand_replace_map, name)\n",
    "        print(\"ğŸ—‘ ÄÃ£ loáº¡i cÃ¡c cá»™t dÆ° thá»«a Ä‘áº·c biá»‡t\")\n",
    "\n",
    "    elif name == \"Brand_Image\":\n",
    "        if 'Attribute' in df_clean.columns:\n",
    "            df_clean['Attribute'] = df_clean['Attribute'].str.lower().str.strip()\n",
    "            df_clean['Attribute'] = df_clean['Attribute'].replace({\n",
    "                'good place for working / business meeting': 'good place for working',\n",
    "                'good place for working / studying': 'good place for working',\n",
    "                'good place for socializing with friends': 'good place for socializing',\n",
    "                'good place for socializing with family': 'good place for socializing',\n",
    "                'good place for socializing with colleagues': 'good place for socializing',\n",
    "                'nice environment design': 'pleasant environment',\n",
    "                'comfortable and relaxing environment': 'pleasant environment',\n",
    "                'good other beverages (other than coffee, tea & ice-blended)': 'good other beverages',\n",
    "                'have new product regularly': 'frequent product updates',\n",
    "                'feel i belong here': 'sense of belonging',\n",
    "                'delicious food': 'good food taste'\n",
    "            })\n",
    "            print(\"ğŸ”§ ÄÃ£ chuáº©n hoÃ¡ cá»™t Attribute\")\n",
    "\n",
    "        for col in ['Awareness', 'BrandImage']:\n",
    "            if col in df_clean.columns:\n",
    "                df_clean[col] = df_clean[col].str.lower().str.strip()\n",
    "                df_clean[col] = df_clean[col].replace(brand_replace_map)\n",
    "                print(f\"ğŸ”§ ÄÃ£ chuáº©n hoÃ¡ cá»™t {col}\")\n",
    "\n",
    "    elif name == \"Brand_Health\":\n",
    "        brand_columns = ['Brand', 'Awareness', 'Spontaneous', 'Trial', 'P3M', 'P1M', 'Weekly', 'Daily']\n",
    "        clean_brand_columns(df_clean, brand_columns, brand_replace_map, name)\n",
    "\n",
    "    # elif name == \"NeedstateDayDaypart\":\n",
    "    #     if 'Day#Daypart' in df_clean.columns:\n",
    "    #         df_daypart_split = df_clean['Day#Daypart'].apply(clean_day_daypart).apply(pd.Series)\n",
    "    #         df_clean = pd.concat([df_clean, df_daypart_split], axis=1)\n",
    "    #         df_clean.drop(columns=['Day#Daypart'], inplace=True)\n",
    "    #         print(\"ğŸ”§ ÄÃ£ tÃ¡ch cá»™t Day#Daypart thÃ nh DayGroup & Daypart_cleaned\")\n",
    "\n",
    "    #     if 'Needstates' in df_clean.columns:\n",
    "    #         df_clean['Needstates_standardized'] = df_clean['Needstates'].apply(standardize_needstate)\n",
    "    #         print(\"ğŸ”§ ÄÃ£ chuáº©n hoÃ¡ cá»™t Needstates\")\n",
    "            \n",
    "    #     df_clean = df_clean.drop(columns=['Needstates'])\n",
    "\n",
    "    # ğŸ’¾ Ghi file ra\n",
    "    output_path = os.path.join(output_dir, f\"{name}_cleaned.csv\")\n",
    "    df_clean.to_csv(output_path, index=False)\n",
    "    print(f\"ğŸ’¾ ÄÃ£ lÆ°u: {name}_cleaned.csv\")\n",
    "\n",
    "# âœ¨ Gá»i xá»­ lÃ½ cho tá»«ng file\n",
    "process_df(df_seg, \"Segmentation\", drop_cols=[])  \n",
    "process_df(df_image, \"Brand_Image\")\n",
    "process_df(df_health, \"Brand_Health\")\n",
    "process_df(df_companion, \"Companion\")\n",
    "process_df(df_competitor, \"Competitor\", drop_cols=[])  \n",
    "process_df(df_dow, \"Dayofweek\")\n",
    "process_df(df_daypart, \"Daypart\")\n",
    "process_df(df_need, \"NeedstateDayDaypart\")\n",
    "process_df(df_sa, \"SA_var\", drop_cols=[])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0bb2aa8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“ File: Brand_Health_cleaned.csv\n",
      "ğŸ“ Tá»•ng sá»‘ dÃ²ng: 74419\n",
      "          Column  Null Count  Null %\n",
      "     Spontaneous       43426   58.35\n",
      "       Awareness         114    0.15\n",
      "           Trial       27089   36.40\n",
      "             P3M       45570   61.23\n",
      "             P1M       55020   73.93\n",
      "   Comprehension       48073   64.60\n",
      "Brand_Likability       64088   86.12\n",
      "          Weekly       61037   82.02\n",
      "           Daily       66798   89.76\n",
      "       Fre#visit       55087   74.02\n",
      "             PPA       60346   81.09\n",
      "        Spending       60346   81.09\n",
      "    Segmentation       60346   81.09\n",
      "         NPS#P3M       52814   70.97\n",
      "   NPS#P3M#Group       52814   70.97\n",
      "    Spending_use       60346   81.09\n",
      "\n",
      "ğŸ“ File: Brand_Image_cleaned.csv\n",
      "ğŸ“ Tá»•ng sá»‘ dÃ²ng: 629098\n",
      "   Column  Null Count  Null %\n",
      "Awareness         397    0.06\n",
      "\n",
      "âœ… File: Companion_cleaned.csv - KHÃ”NG cÃ³ giÃ¡ trá»‹ null\n",
      "ğŸ“ Tá»•ng sá»‘ dÃ²ng: 19940\n",
      "\n",
      "âœ… File: Competitor_cleaned.csv - KHÃ”NG cÃ³ giÃ¡ trá»‹ null\n",
      "ğŸ“ Tá»•ng sá»‘ dÃ²ng: 234\n",
      "\n",
      "ğŸ“ File: Dayofweek_cleaned.csv\n",
      "ğŸ“ Tá»•ng sá»‘ dÃ²ng: 39058\n",
      "         Column  Null Count  Null %\n",
      "      Dayofweek          49    0.13\n",
      "Visit#Dayofweek          54    0.14\n",
      "    Weekday#end          49    0.13\n",
      "\n",
      "ğŸ“ File: Daypart_cleaned.csv\n",
      "ğŸ“ Tá»•ng sá»‘ dÃ²ng: 19189\n",
      "       Column  Null Count  Null %\n",
      "      Daypart          13    0.07\n",
      "Visit#Daypart         847    4.41\n",
      "\n",
      "âœ… File: NeedstateDayDaypart_cleaned.csv - KHÃ”NG cÃ³ giÃ¡ trá»‹ null\n",
      "ğŸ“ Tá»•ng sá»‘ dÃ²ng: 75179\n",
      "\n",
      "ğŸ“ File: SA_var_cleaned.csv\n",
      "ğŸ“ Tá»•ng sá»‘ dÃ²ng: 11761\n",
      "       Column  Null Count  Null %\n",
      "   Group_size          15    0.13\n",
      "          Age           9    0.08\n",
      "     MPI#Mean        3717   31.60\n",
      "BUMO_Previous        5665   48.17\n",
      "  Age#Group#2           9    0.08\n",
      "        MPI#2        3717   31.60\n",
      "\n",
      "âœ… File: Segmentation_cleaned.csv - KHÃ”NG cÃ³ giÃ¡ trá»‹ null\n",
      "ğŸ“ Tá»•ng sá»‘ dÃ²ng: 4944\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# ğŸ“‚ ThÆ° má»¥c chá»©a file Ä‘Ã£ xá»­ lÃ½\n",
    "cleaned_dir = \"./cleaned_data/\"\n",
    "\n",
    "# ğŸ“Š HÃ m kiá»ƒm tra null cho tá»«ng cá»™t vÃ  in tá»•ng sá»‘ dÃ²ng\n",
    "def report_nulls(file_path):\n",
    "    df = pd.read_csv(file_path, sep=',', low_memory=False)\n",
    "    total_rows = df.shape[0]\n",
    "    null_info = df.isnull().sum()\n",
    "    null_percent = (null_info / total_rows * 100).round(2)\n",
    "    \n",
    "    null_df = pd.DataFrame({\n",
    "        \"Column\": null_info.index,\n",
    "        \"Null Count\": null_info.values,\n",
    "        \"Null %\": null_percent.values\n",
    "    })\n",
    "    \n",
    "    null_df = null_df[null_df[\"Null Count\"] > 0]\n",
    "\n",
    "    file_name = os.path.basename(file_path)\n",
    "    \n",
    "    if not null_df.empty:\n",
    "        print(f\"\\nğŸ“ File: {file_name}\")\n",
    "        print(f\"ğŸ“ Tá»•ng sá»‘ dÃ²ng: {total_rows}\")\n",
    "        print(null_df.to_string(index=False))\n",
    "    else:\n",
    "        print(f\"\\nâœ… File: {file_name} - KHÃ”NG cÃ³ giÃ¡ trá»‹ null\")\n",
    "        print(f\"ğŸ“ Tá»•ng sá»‘ dÃ²ng: {total_rows}\")\n",
    "\n",
    "# ğŸš€ QuÃ©t táº¥t cáº£ file CSV Ä‘Ã£ xá»­ lÃ½\n",
    "for fname in os.listdir(cleaned_dir):\n",
    "    if fname.endswith(\".csv\"):\n",
    "        report_nulls(os.path.join(cleaned_dir, fname))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9033a91a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ Sá»‘ dÃ²ng ban Ä‘áº§u: 39058\n",
      "âœ… Sá»‘ dÃ²ng sau khi loáº¡i bá» dÃ²ng null: 38955\n",
      "ğŸ—‘ï¸ ÄÃ£ loáº¡i bá» 36296 dÃ²ng chá»©a giÃ¡ trá»‹ null\n",
      "âœ… ÄÃ£ xá»­ lÃ½ vÃ  lÆ°u láº¡i táº¡i: cleaned_data/SA_var_cleaned.csv\n",
      "ğŸ“ File: Brand_Image_cleaned.csv\n",
      "ğŸ“ Tá»•ng sá»‘ dÃ²ng: 629098\n",
      "   Column  Null Count  Null %\n",
      "Awareness 397    0.06%\n",
      "âœ… ÄÃ£ xá»­ lÃ½ vÃ  lÆ°u láº¡i táº¡i: ./cleaned_data/Brand_Image_cleaned.csv\n",
      "\n",
      "ğŸ“ File: Daypart_cleaned.csv\n",
      "ğŸ“ Tá»•ng sá»‘ dÃ²ng: 19189\n",
      "ğŸ§­ Daypart missing: 13 (0.07%)\n",
      "ğŸ§­ Visit#Daypart missing: 847 (4.41%)\n",
      "âœ… ÄÃ£ xá»­ lÃ½ vÃ  lÆ°u láº¡i táº¡i: ./cleaned_data/Daypart_cleaned.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\phamh\\AppData\\Local\\Temp\\ipykernel_22736\\481900124.py:122: DtypeWarning: Columns (2,5,6,8,9,10,14,16) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“ File: Brand_Health_cleaned.csv\n",
      "ğŸ“ Tá»•ng sá»‘ dÃ²ng: 74419\n",
      "ğŸ§­ Spontaneous missing: 43426 (58.35%)\n",
      "ğŸ§­ Awareness missing: 114 (0.15%)\n",
      "âœ… Awareness sau xá»­ lÃ½: cÃ²n 0 dÃ²ng thiáº¿u.\n",
      "âœ… ÄÃ£ xá»­ lÃ½ vÃ  lÆ°u láº¡i táº¡i: ./cleaned_data/Brand_Health_cleaned.csv\n",
      "âœ… Sau khi ghi Ä‘Ã¨ theo logic:\n",
      "Trial = 1: 47337 / 74419\n",
      "P3M   = 1: 28852 / 74419\n",
      "P1M   = 1: 19399 / 74419\n",
      "ğŸ’¾ ÄÃ£ ghi Ä‘Ã¨ file: Brand_Health_cleaned.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\phamh\\AppData\\Local\\Temp\\ipykernel_22736\\481900124.py:181: DtypeWarning: Columns (8,9,10,14,16) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(\"cleaned_data/Brand_Health_cleaned.csv\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ÄÃ£ xá»­ lÃ½ thÃ nh cÃ´ng!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ğŸ”¹ BÆ°á»›c 1: LÃ m sáº¡ch Dayofweek_cleaned.csv\n",
    "df_day = pd.read_csv(\"./cleaned_data/Dayofweek_cleaned.csv\")\n",
    "print(f\"ğŸ“ Sá»‘ dÃ²ng ban Ä‘áº§u: {df_day.shape[0]}\")\n",
    "\n",
    "df_day = df_day.dropna()\n",
    "\n",
    "print(f\"âœ… Sá»‘ dÃ²ng sau khi loáº¡i bá» dÃ²ng null: {df_day.shape[0]}\")\n",
    "print(f\"ğŸ—‘ï¸ ÄÃ£ loáº¡i bá» {df.shape[0] - df_day.shape[0]} dÃ²ng chá»©a giÃ¡ trá»‹ null\")\n",
    "\n",
    "# Ghi Ä‘Ã¨ láº¡i file\n",
    "df_day.to_csv(\"./cleaned_data/Dayofweek_cleaned.csv\", index=False)\n",
    "\n",
    "\n",
    "# ğŸ”¹ BÆ°á»›c 2: LÃ m sáº¡ch SA_var_cleaned.csv\n",
    "df = pd.read_csv(\"cleaned_data/SA_var_cleaned.csv\")\n",
    "\n",
    "# Äiá»n thiáº¿u Group_size vÃ  Age báº±ng median\n",
    "df[\"Group_size\"] = df[\"Group_size\"].fillna(df[\"Group_size\"].median())\n",
    "df[\"Age\"] = df[\"Age\"].fillna(df[\"Age\"].median())\n",
    "\n",
    "# Táº¡o nhÃ³m tuá»•i tá»« Age\n",
    "def map_age_group(age):\n",
    "    if age <= 19:\n",
    "        return '16 - 19 y.o.'\n",
    "    elif age <= 24:\n",
    "        return '20 - 24 y.o.'\n",
    "    elif age <= 29:\n",
    "        return '25 - 29 y.o.'\n",
    "    elif age <= 34:\n",
    "        return '30 - 34 y.o.'\n",
    "    elif age <= 39:\n",
    "        return '35 - 39 y.o.'\n",
    "    elif age <= 44:\n",
    "        return '40 - 44 y.o.'\n",
    "    else:\n",
    "        return '45+ y.o.'\n",
    "\n",
    "df[\"Age#Group#2\"] = df[\"Age\"].apply(map_age_group)\n",
    "\n",
    "# Äiá»n thiáº¿u MPI#Mean theo nhÃ³m tuá»•i, rá»“i toÃ n bá»™\n",
    "df[\"MPI#Mean\"] = df.groupby(\"Age#Group#2\")[\"MPI#Mean\"].transform(lambda x: x.fillna(x.median()))\n",
    "df[\"MPI#Mean\"] = df[\"MPI#Mean\"].fillna(df[\"MPI#Mean\"].median())\n",
    "\n",
    "# Suy ra MPI#2 tá»« MPI#Mean\n",
    "def map_mpi_category(mpi):\n",
    "    if mpi < 4500:\n",
    "        return \"1.Under VND 4.5m\"\n",
    "    elif mpi < 9000:\n",
    "        return \"2.VND 4.5m - VND 8.9m\"\n",
    "    elif mpi < 15000:\n",
    "        return \"3.VND 9m - VND 14.9m\"\n",
    "    elif mpi < 25000:\n",
    "        return \"4.VND 15m - VND 24.9m\"\n",
    "    else:\n",
    "        return \"5.VND 25m+\"\n",
    "\n",
    "df[\"MPI#2\"] = df[\"MPI#Mean\"].apply(map_mpi_category)\n",
    "\n",
    "# Thay null á»Ÿ cá»™t BUMO_Previous báº±ng \"No\"\n",
    "df[\"BUMO_Previous\"] = df[\"BUMO_Previous\"].fillna(\"No\")\n",
    "\n",
    "# Ghi Ä‘Ã¨ láº¡i file\n",
    "df.to_csv(\"cleaned_data/SA_var_cleaned.csv\", index=False)\n",
    "print(\"âœ… ÄÃ£ xá»­ lÃ½ vÃ  lÆ°u láº¡i táº¡i: cleaned_data/SA_var_cleaned.csv\")\n",
    "\n",
    "\n",
    "# ğŸ”¹ BÆ°á»›c 3: LÃ m sáº¡ch Brand_Image_cleaned.csv vá»›i xá»­ lÃ½ há»£p lÃ½\n",
    "file_path = \"./cleaned_data/Brand_Image_cleaned.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# In thá»‘ng kÃª ban Ä‘áº§u\n",
    "total_rows = df.shape[0]\n",
    "null_count = df[\"Awareness\"].isnull().sum()\n",
    "print(f\"ğŸ“ File: Brand_Image_cleaned.csv\")\n",
    "print(f\"ğŸ“ Tá»•ng sá»‘ dÃ²ng: {total_rows}\")\n",
    "print(f\"   Column  Null Count  Null %\")\n",
    "print(f\"Awareness {null_count}    {round(100 * null_count / total_rows, 2)}%\")\n",
    "\n",
    "# GÃ¡n cáº©n tháº­n: náº¿u Awareness null mÃ  BrandImage cÃ³ giÃ¡ trá»‹ â†’ suy luáº­n tá»« BrandImage\n",
    "df.loc[df[\"Awareness\"].isnull() & df[\"BrandImage\"].notnull(), \"Awareness\"] = df[\"BrandImage\"]\n",
    "\n",
    "# Náº¿u váº«n cÃ²n null, tá»©c lÃ  thá»±c sá»± \"Unaware\"\n",
    "df[\"Awareness\"] = df[\"Awareness\"].fillna(\"Unaware\")\n",
    "\n",
    "# Ghi Ä‘Ã¨ láº¡i file\n",
    "df.to_csv(file_path, index=False)\n",
    "print(f\"âœ… ÄÃ£ xá»­ lÃ½ vÃ  lÆ°u láº¡i táº¡i: {file_path}\")\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# ğŸ”¹ BÆ°á»›c 4: LÃ m sáº¡ch Daypart_cleaned.csv\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "file_path = \"./cleaned_data/Daypart_cleaned.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "print(\"\\nğŸ“ File: Daypart_cleaned.csv\")\n",
    "print(f\"ğŸ“ Tá»•ng sá»‘ dÃ²ng: {df.shape[0]}\")\n",
    "\n",
    "# Thá»‘ng kÃª null\n",
    "daypart_null = df[\"Daypart\"].isnull().sum()\n",
    "visit_null = df[\"Visit#Daypart\"].isnull().sum()\n",
    "print(f\"ğŸ§­ Daypart missing: {daypart_null} ({round(100 * daypart_null / df.shape[0], 2)}%)\")\n",
    "print(f\"ğŸ§­ Visit#Daypart missing: {visit_null} ({round(100 * visit_null / df.shape[0], 2)}%)\")\n",
    "\n",
    "# ğŸ”¸ Náº¿u chá»‰ thiáº¿u Daypart, Ä‘iá»n báº±ng mode (giÃ¡ trá»‹ phá»• biáº¿n nháº¥t)\n",
    "most_common_daypart = df[\"Daypart\"].mode()[0]\n",
    "df[\"Daypart\"] = df[\"Daypart\"].fillna(most_common_daypart)\n",
    "\n",
    "# ğŸ”¸ Äiá»n thiáº¿u Visit#Daypart báº±ng median theo tá»«ng nhÃ³m Daypart\n",
    "df[\"Visit#Daypart\"] = df.groupby(\"Daypart\")[\"Visit#Daypart\"].transform(lambda x: x.fillna(x.median()))\n",
    "\n",
    "# ğŸ”¸ Náº¿u cÃ²n giÃ¡ trá»‹ thiáº¿u do Daypart lÃ  null trÆ°á»›c Ä‘Ã³ â†’ fallback báº±ng median toÃ n bá»™\n",
    "df[\"Visit#Daypart\"] = df[\"Visit#Daypart\"].fillna(df[\"Visit#Daypart\"].median())\n",
    "\n",
    "# ğŸ”¸ Ghi láº¡i file sau xá»­ lÃ½\n",
    "df.to_csv(file_path, index=False)\n",
    "print(f\"âœ… ÄÃ£ xá»­ lÃ½ vÃ  lÆ°u láº¡i táº¡i: {file_path}\")\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# ğŸ”¹ BÆ°á»›c 5: LÃ m sáº¡ch Brand_Health_cleaned.csv â€“ Spontaneous & Awareness\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "file_path = \"./cleaned_data/Brand_Health_cleaned.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "print(\"\\nğŸ“ File: Brand_Health_cleaned.csv\")\n",
    "print(f\"ğŸ“ Tá»•ng sá»‘ dÃ²ng: {df.shape[0]}\")\n",
    "\n",
    "# ğŸ§­ Thá»‘ng kÃª missing\n",
    "spontaneous_null = df[\"Spontaneous\"].isnull().sum()\n",
    "awareness_null = df[\"Awareness\"].isnull().sum()\n",
    "print(f\"ğŸ§­ Spontaneous missing: {spontaneous_null} ({round(100 * spontaneous_null / df.shape[0], 2)}%)\")\n",
    "print(f\"ğŸ§­ Awareness missing: {awareness_null} ({round(100 * awareness_null / df.shape[0], 2)}%)\")\n",
    "\n",
    "# ğŸ”¸ GÃ¡n cáº©n tháº­n: náº¿u Awareness bá»‹ thiáº¿u nhÆ°ng Spontaneous cÃ³ giÃ¡ trá»‹ â†’ gÃ¡n Awareness = Spontaneous\n",
    "df.loc[df[\"Awareness\"].isnull() & df[\"Spontaneous\"].notnull(), \"Awareness\"] = df[\"Spontaneous\"]\n",
    "\n",
    "# ğŸ”¸ Sau khi suy luáº­n, náº¿u váº«n thiáº¿u Awareness â†’ gÃ¡n lÃ  \"Unaware\"\n",
    "df[\"Awareness\"] = df[\"Awareness\"].fillna(\"Unaware\")\n",
    "\n",
    "# ğŸ”¸ Ghi log cáº­p nháº­t láº¡i sá»‘ null sau xá»­ lÃ½\n",
    "remaining_awareness_null = df[\"Awareness\"].isnull().sum()\n",
    "print(f\"âœ… Awareness sau xá»­ lÃ½: cÃ²n {remaining_awareness_null} dÃ²ng thiáº¿u.\")\n",
    "\n",
    "# ğŸ”¸ Sau khi suy luáº­n, náº¿u váº«n thiáº¿u Awareness â†’ gÃ¡n lÃ  \"Unaware\"\n",
    "df[\"Spontaneous\"] = df[\"Spontaneous\"].fillna(\"Unaware\")\n",
    "\n",
    "# ğŸ”¸ Ghi láº¡i file\n",
    "df.to_csv(file_path, index=False)\n",
    "print(f\"âœ… ÄÃ£ xá»­ lÃ½ vÃ  lÆ°u láº¡i táº¡i: {file_path}\")\n",
    "import pandas as pd\n",
    "\n",
    "# Äá»c file vÃ  lÃ m sáº¡ch tÃªn cá»™t\n",
    "df = pd.read_csv(\"./cleaned_data/Brand_Health_cleaned.csv\", low_memory=False)\n",
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "# BÆ°á»›c 1: Chuyá»ƒn cÃ¡c cá»™t thÃ nh nhá»‹ phÃ¢n (1 náº¿u cÃ³ giÃ¡ trá»‹, 0 náº¿u NaN)\n",
    "df[\"Trial\"] = df[\"Trial\"].notnull().astype(int)\n",
    "df[\"P3M\"] = df[\"P3M\"].notnull().astype(int)\n",
    "df[\"P1M\"] = df[\"P1M\"].notnull().astype(int)\n",
    "\n",
    "# BÆ°á»›c 2: Ãp dá»¥ng logic:\n",
    "# Náº¿u P3M = 1 â†’ cháº¯c cháº¯n Trial = 1\n",
    "df.loc[(df[\"P3M\"] == 1) & (df[\"Trial\"] == 0), \"Trial\"] = 1\n",
    "\n",
    "# Náº¿u P1M = 1 â†’ cháº¯c cháº¯n Trial = 1 vÃ  P3M = 1\n",
    "df.loc[df[\"P1M\"] == 1, \"Trial\"] = 1\n",
    "df.loc[(df[\"P1M\"] == 1) & (df[\"P3M\"] == 0), \"P3M\"] = 1\n",
    "\n",
    "# Thá»‘ng kÃª sau xá»­ lÃ½\n",
    "print(\"âœ… Sau khi ghi Ä‘Ã¨ theo logic:\")\n",
    "print(f\"Trial = 1: {(df['Trial'] == 1).sum()} / {len(df)}\")\n",
    "print(f\"P3M   = 1: {(df['P3M'] == 1).sum()} / {len(df)}\")\n",
    "print(f\"P1M   = 1: {(df['P1M'] == 1).sum()} / {len(df)}\")\n",
    "\n",
    "# Ghi Ä‘Ã¨ láº¡i file gá»‘c\n",
    "df.to_csv(\"./cleaned_data/Brand_Health_cleaned.csv\", index=False)\n",
    "print(\"ğŸ’¾ ÄÃ£ ghi Ä‘Ã¨ file: Brand_Health_cleaned.csv\")\n",
    "\n",
    "\n",
    "# Giáº£ sá»­ báº¡n Ä‘Ã£ load báº£ng vÃ o df\n",
    "# Load dá»¯ liá»‡u\n",
    "df = pd.read_csv(\"cleaned_data/Brand_Health_cleaned.csv\")\n",
    "\n",
    "# ğŸ”§ Chuáº©n hÃ³a tÃªn cá»™t\n",
    "df.columns = df.columns.str.strip().str.lower().str.replace('#', '_')\n",
    "\n",
    "# ğŸ”¹ 1. Náº¿u fre_visit vÃ  spending Ä‘á»u cÃ³ â†’ tÃ­nh láº¡i PPA\n",
    "mask1 = df[\"fre_visit\"].notnull() & df[\"spending\"].notnull()\n",
    "df.loc[mask1, \"ppa\"] = df.loc[mask1, \"spending\"] / df.loc[mask1, \"fre_visit\"]\n",
    "\n",
    "# ğŸ”¹ 2. Náº¿u fre_visit lÃ  NA â†’ gÃ¡n cáº£ 4 cá»™t = 0\n",
    "mask2 = df[\"fre_visit\"].isnull()\n",
    "df.loc[mask2, [\"fre_visit\", \"ppa\", \"spending\", \"spending_use\"]] = 0\n",
    "\n",
    "# ğŸ”¹ 3. Náº¿u cÃ³ 2/3 â†’ tÃ­nh pháº§n cÃ²n láº¡i\n",
    "# fre_visit + ppa â†’ spending\n",
    "mask3 = df[\"fre_visit\"].notnull() & df[\"ppa\"].notnull() & df[\"spending\"].isnull()\n",
    "df.loc[mask3, \"spending\"] = df.loc[mask3, \"fre_visit\"] * df.loc[mask3, \"ppa\"]\n",
    "\n",
    "# fre_visit + spending â†’ ppa\n",
    "mask4 = df[\"fre_visit\"].notnull() & df[\"spending\"].notnull() & df[\"ppa\"].isnull()\n",
    "df.loc[mask4, \"ppa\"] = df.loc[mask4, \"spending\"] / df.loc[mask4, \"fre_visit\"]\n",
    "\n",
    "# ppa + spending â†’ fre_visit\n",
    "mask5 = df[\"ppa\"].notnull() & df[\"spending\"].notnull() & df[\"fre_visit\"].isnull()\n",
    "df.loc[mask5, \"fre_visit\"] = df.loc[mask5, \"spending\"] / df.loc[mask5, \"ppa\"]\n",
    "\n",
    "# ğŸ”¹ 4. Náº¿u spending_use lÃ  NA â†’ gÃ¡n báº±ng spending\n",
    "df[\"spending_use\"] = df[\"spending_use\"].fillna(df[\"spending\"])\n",
    "# GÃ¡n ppa cÃ²n thiáº¿u báº±ng median toÃ n bá»™ sample\n",
    "ppa_median = df[\"ppa\"].median()\n",
    "df[\"ppa\"] = df[\"ppa\"].fillna(ppa_median)\n",
    "\n",
    "# TÃ­nh láº¡i spending = ppa * fre_visit náº¿u spending cÃ²n thiáº¿u\n",
    "mask_spending_missing = df[\"spending\"].isnull()\n",
    "df.loc[mask_spending_missing, \"spending\"] = df.loc[mask_spending_missing, \"ppa\"] * df.loc[mask_spending_missing, \"fre_visit\"]\n",
    "\n",
    "# GÃ¡n spending_use báº±ng spending náº¿u cÃ²n thiáº¿u\n",
    "df[\"spending_use\"] = df[\"spending_use\"].fillna(df[\"spending\"])\n",
    "\n",
    "# âœ… LÆ°u láº¡i\n",
    "df.to_csv(\"cleaned_data/Brand_Health_cleaned.csv\", index=False)\n",
    "print(\"âœ… ÄÃ£ xá»­ lÃ½ thÃ nh cÃ´ng!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a494f291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Kiá»ƒm tra cÃ¡c logic violations...\n",
      "\n",
      "â— Vi pháº¡m 1: P1M = 1 nhÆ°ng P3M != 1: 0 dÃ²ng\n",
      "â— Vi pháº¡m 2: P3M = 1 nhÆ°ng Trial != 1: 0 dÃ²ng\n",
      "â— Vi pháº¡m 3: Spending > 0 nhÆ°ng Fre_Visit thiáº¿u hoáº·c báº±ng 0: 0 dÃ²ng\n",
      "â— Vi pháº¡m 4: Fre_Visit > 0 nhÆ°ng PPA thiáº¿u: 0 dÃ²ng\n",
      "â— Vi pháº¡m 5: Fre_Visit > 0 nhÆ°ng Spending thiáº¿u: 0 dÃ²ng\n",
      "â— Vi pháº¡m 6: Spending > 0 nhÆ°ng PPA thiáº¿u: 0 dÃ²ng\n",
      "â— Vi pháº¡m 7: CÃ³ NPS nhÆ°ng Trial hoáº·c P3M = 0: 0 dÃ²ng\n",
      "â— Vi pháº¡m 8: CÃ³ Comprehension/Likability nhÆ°ng Awareness = 'Unaware': 4 dÃ²ng\n",
      "â— Vi pháº¡m 9: CÃ³ Spontaneous nhÆ°ng Awareness = 'Unaware': 90 dÃ²ng\n",
      "â— Vi pháº¡m 10: Spending_use bá»‹ thiáº¿u: 0 dÃ²ng\n",
      "âœ… ÄÃ£ sá»­a 4 dÃ²ng vi pháº¡m 8 (comprehension/likability â†’ nhÆ°ng awareness = 'Unaware')\n",
      "âœ… ÄÃ£ sá»­a 82 dÃ²ng vi pháº¡m 9 (spontaneous cÃ³ nhÆ°ng awareness = 'Unaware')\n",
      "ğŸ’¾ ÄÃ£ ghi Ä‘Ã¨ file: Brand_Health_cleaned.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv(\"cleaned_data/Brand_Health_cleaned.csv\", low_memory=False)\n",
    "df.columns = df.columns.str.strip().str.lower().str.replace('#', '_')\n",
    "\n",
    "print(\"ğŸ” Kiá»ƒm tra cÃ¡c logic violations...\\n\")\n",
    "\n",
    "# 1. P1M = 1 nhÆ°ng P3M != 1\n",
    "violation_1 = df[(df[\"p1m\"] == 1) & (df[\"p3m\"] != 1)]\n",
    "print(f\"â— Vi pháº¡m 1: P1M = 1 nhÆ°ng P3M != 1: {len(violation_1)} dÃ²ng\")\n",
    "\n",
    "# 2. P3M = 1 nhÆ°ng Trial != 1\n",
    "violation_2 = df[(df[\"p3m\"] == 1) & (df[\"trial\"] != 1)]\n",
    "print(f\"â— Vi pháº¡m 2: P3M = 1 nhÆ°ng Trial != 1: {len(violation_2)} dÃ²ng\")\n",
    "\n",
    "# 3. Spending > 0 nhÆ°ng Fre_Visit = 0 hoáº·c NA\n",
    "violation_3 = df[(df[\"spending\"] > 0) & ((df[\"fre_visit\"].isnull()) | (df[\"fre_visit\"] == 0))]\n",
    "print(f\"â— Vi pháº¡m 3: Spending > 0 nhÆ°ng Fre_Visit thiáº¿u hoáº·c báº±ng 0: {len(violation_3)} dÃ²ng\")\n",
    "\n",
    "# 4. Fre_Visit > 0 nhÆ°ng PPA thiáº¿u\n",
    "violation_4 = df[(df[\"fre_visit\"] > 0) & (df[\"ppa\"].isnull())]\n",
    "print(f\"â— Vi pháº¡m 4: Fre_Visit > 0 nhÆ°ng PPA thiáº¿u: {len(violation_4)} dÃ²ng\")\n",
    "\n",
    "# 5. Fre_Visit > 0 nhÆ°ng Spending thiáº¿u\n",
    "violation_5 = df[(df[\"fre_visit\"] > 0) & (df[\"spending\"].isnull())]\n",
    "print(f\"â— Vi pháº¡m 5: Fre_Visit > 0 nhÆ°ng Spending thiáº¿u: {len(violation_5)} dÃ²ng\")\n",
    "\n",
    "# 6. Spending > 0 nhÆ°ng PPA thiáº¿u\n",
    "violation_6 = df[(df[\"spending\"] > 0) & (df[\"ppa\"].isnull())]\n",
    "print(f\"â— Vi pháº¡m 6: Spending > 0 nhÆ°ng PPA thiáº¿u: {len(violation_6)} dÃ²ng\")\n",
    "\n",
    "# 7. NPS cÃ³ nhÆ°ng Trial hoáº·c P3M = 0\n",
    "violation_7 = df[(df[\"nps_p3m\"].notnull()) & ((df[\"trial\"] != 1) | (df[\"p3m\"] != 1))]\n",
    "print(f\"â— Vi pháº¡m 7: CÃ³ NPS nhÆ°ng Trial hoáº·c P3M = 0: {len(violation_7)} dÃ²ng\")\n",
    "\n",
    "# 8. CÃ³ Comprehension hoáº·c Brand_Likability nhÆ°ng Awareness = 'Unaware'\n",
    "violation_8 = df[\n",
    "    ((df[\"comprehension\"].notnull()) | (df[\"brand_likability\"].notnull())) &\n",
    "    (df[\"awareness\"].str.lower() == \"unaware\")\n",
    "]\n",
    "print(f\"â— Vi pháº¡m 8: CÃ³ Comprehension/Likability nhÆ°ng Awareness = 'Unaware': {len(violation_8)} dÃ²ng\")\n",
    "\n",
    "# 9. Spontaneous cÃ³ nhÆ°ng Awareness = 'Unaware'\n",
    "violation_9 = df[(df[\"spontaneous\"].notnull()) & (df[\"awareness\"].str.lower() == \"unaware\")]\n",
    "print(f\"â— Vi pháº¡m 9: CÃ³ Spontaneous nhÆ°ng Awareness = 'Unaware': {len(violation_9)} dÃ²ng\")\n",
    "\n",
    "# 10. Spending_use bá»‹ thiáº¿u\n",
    "violation_10 = df[df[\"spending_use\"].isnull()]\n",
    "print(f\"â— Vi pháº¡m 10: Spending_use bá»‹ thiáº¿u: {len(violation_10)} dÃ²ng\")\n",
    "import pandas as pd\n",
    "\n",
    "# Äá»c dá»¯ liá»‡u\n",
    "df = pd.read_csv(\"cleaned_data/Brand_Health_cleaned.csv\", low_memory=False)\n",
    "\n",
    "# Chuáº©n hÃ³a tÃªn cá»™t\n",
    "df.columns = df.columns.str.strip().str.lower().str.replace('#', '_')\n",
    "\n",
    "# Táº¡o cá» ban Ä‘áº§u náº¿u chÆ°a cÃ³\n",
    "if \"was_awareness_corrected\" not in df.columns:\n",
    "    df[\"was_awareness_corrected\"] = 0\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# ğŸ”¹ Vi pháº¡m 8: CÃ³ comprehension/likability nhÆ°ng awareness = \"Unaware\"\n",
    "mask8 = (\n",
    "    ((df[\"comprehension\"].notnull()) | (df[\"brand_likability\"].notnull())) &\n",
    "    (df[\"awareness\"].str.lower() == \"unaware\")\n",
    ")\n",
    "\n",
    "# GÃ¡n láº¡i thÃ nh \"Aware\"\n",
    "df.loc[mask8, \"awareness\"] = \"Aware\"\n",
    "df.loc[mask8, \"was_awareness_corrected\"] = 1\n",
    "print(f\"âœ… ÄÃ£ sá»­a {mask8.sum()} dÃ²ng vi pháº¡m 8 (comprehension/likability â†’ nhÆ°ng awareness = 'Unaware')\")\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# ğŸ”¹ Vi pháº¡m 9: CÃ³ spontaneous nhÆ°ng awareness = \"Unaware\"\n",
    "mask9 = (df[\"spontaneous\"].notnull()) & (df[\"awareness\"].str.lower() == \"unaware\")\n",
    "\n",
    "# GÃ¡n láº¡i awareness = spontaneous\n",
    "df.loc[mask9, \"awareness\"] = df.loc[mask9, \"spontaneous\"]\n",
    "df.loc[mask9, \"was_awareness_corrected\"] = 1\n",
    "print(f\"âœ… ÄÃ£ sá»­a {mask9.sum()-4} dÃ²ng vi pháº¡m 9 (spontaneous cÃ³ nhÆ°ng awareness = 'Unaware')\")\n",
    "\n",
    "# Ghi láº¡i káº¿t quáº£\n",
    "df.to_csv(\"cleaned_data/Brand_Health_cleaned.csv\", index=False)\n",
    "print(\"ğŸ’¾ ÄÃ£ ghi Ä‘Ã¨ file: Brand_Health_cleaned.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cdc5e505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â— CÃ³ 0 dÃ²ng cÃ³ Needstates nhÆ°ng thiáº¿u NeedstateGroup\n",
      "\n",
      "ğŸ” Kiá»ƒm tra logic báº£ng: Competitor Database\n",
      "â— DÃ²ng cÃ³ storecount < 0 hoáº·c thiáº¿u: 0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Äá»c file\n",
    "df = pd.read_csv(\"cleaned_data/NeedstateDayDaypart_cleaned.csv\")\n",
    "df.columns = df.columns.str.strip().str.lower().str.replace('#', '_')\n",
    "\n",
    "# 2ï¸âƒ£ Needstates cÃ³ nhÆ°ng thiáº¿u needstategroup\n",
    "needstate_logic_mismatch = df[(df[\"needstates\"].notnull()) & (df[\"needstategroup\"].isnull())]\n",
    "print(f\"â— CÃ³ {len(needstate_logic_mismatch)} dÃ²ng cÃ³ Needstates nhÆ°ng thiáº¿u NeedstateGroup\")\n",
    "\n",
    "\n",
    "# Äá»c dá»¯ liá»‡u\n",
    "df = pd.read_csv(\"cleaned_data/Competitor_cleaned.csv\")\n",
    "df.columns = df.columns.str.strip().str.lower().str.replace('#', '_')\n",
    "\n",
    "print(\"\\nğŸ” Kiá»ƒm tra logic báº£ng: Competitor Database\")\n",
    "\n",
    "# 1ï¸âƒ£ storecount pháº£i lÃ  sá»‘ nguyÃªn vÃ  khÃ´ng Ã¢m\n",
    "storecount_invalid = df[(df[\"storecount\"] < 0) | (df[\"storecount\"].isnull())]\n",
    "print(f\"â— DÃ²ng cÃ³ storecount < 0 hoáº·c thiáº¿u: {len(storecount_invalid)}\")\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Äá»c dá»¯ liá»‡u\n",
    "dayofweek = pd.read_csv(\"cleaned_data/Dayofweek_cleaned.csv\")\n",
    "daypart = pd.read_csv(\"cleaned_data/Daypart_cleaned.csv\")\n",
    "\n",
    "# Chuáº©n hÃ³a tÃªn cá»™t\n",
    "dayofweek.columns = dayofweek.columns.str.strip().str.lower().str.replace('#', '_')\n",
    "daypart.columns = daypart.columns.str.strip().str.lower().str.replace('#', '_')\n",
    "\n",
    "# ----------- Kiá»ƒm tra Dayofweek.csv -----------\n",
    "\n",
    "# 1ï¸âƒ£ Kiá»ƒm tra dayofweek cÃ³ Ä‘Ãºng tÃªn thá»© khÃ´ng\n",
    "valid_days = {\"monday\", \"tuesday\", \"wednesday\", \"thursday\", \"friday\", \"saturday\", \"sunday\"}\n",
    "invalid_day_names = dayofweek[~dayofweek[\"dayofweek\"].str.lower().isin(valid_days)]\n",
    "\n",
    "# 2ï¸âƒ£ Kiá»ƒm tra visit_dayofweek cÃ³ giÃ¡ trá»‹ khÃ´ng Ã¢m vÃ  khÃ´ng null\n",
    "invalid_visits = dayofweek[(dayofweek[\"visit_dayofweek\"].isnull()) | (dayofweek[\"visit_dayofweek\"] < 0)]\n",
    "\n",
    "# 3ï¸âƒ£ Kiá»ƒm tra weakday_end cÃ³ khá»›p vá»›i dayofweek\n",
    "def map_weekgroup(day):\n",
    "    return \"Weekend\" if day.lower() in [\"saturday\", \"sunday\"] else \"Weekdays\"\n",
    "\n",
    "dayofweek[\"expected_group\"] = dayofweek[\"dayofweek\"].str.lower().apply(map_weekgroup)\n",
    "wrong_group = dayofweek[dayofweek[\"weekday_end\"].str.lower() != dayofweek[\"expected_group\"].str.lower()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9b5c4692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âŒ Group_size khÃ´ng há»£p lá»‡:\n",
      "Empty DataFrame\n",
      "Columns: [id, group_size]\n",
      "Index: [] \n",
      "\n",
      "âŒ Age group khÃ´ng khá»›p vá»›i Age:\n",
      "Empty DataFrame\n",
      "Columns: [id, age, age_group_2, expected_age_group]\n",
      "Index: [] \n",
      "\n",
      "âŒ MPI group khÃ´ng khá»›p vá»›i MPI_Mean:\n",
      "Empty DataFrame\n",
      "Columns: [id, mpi_mean, mpi_2, expected_mpi_2]\n",
      "Index: [] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Äá»c dá»¯ liá»‡u\n",
    "df = pd.read_csv(\"cleaned_data/SA_var_cleaned.csv\")\n",
    "df.columns = df.columns.str.strip().str.lower().str.replace('#', '_')\n",
    "\n",
    "# ğŸ”¹ Group_size khÃ´ng há»£p lá»‡\n",
    "invalid_group_size = df[(df[\"group_size\"].isnull()) | (df[\"group_size\"] < 1)]\n",
    "print(\"âŒ Group_size khÃ´ng há»£p lá»‡:\")\n",
    "print(invalid_group_size[[\"id\", \"group_size\"]].head(15), \"\\n\")\n",
    "\n",
    "# ğŸ”¹ Age group khÃ´ng khá»›p vá»›i tuá»•i\n",
    "def map_age_group(age):\n",
    "    if age <= 19: return '16 - 19 y.o.'\n",
    "    elif age <= 24: return '20 - 24 y.o.'\n",
    "    elif age <= 29: return '25 - 29 y.o.'\n",
    "    elif age <= 34: return '30 - 34 y.o.'\n",
    "    elif age <= 39: return '35 - 39 y.o.'\n",
    "    elif age <= 44: return '40 - 44 y.o.'\n",
    "    else: return '45+ y.o.'\n",
    "\n",
    "df[\"expected_age_group\"] = df[\"age\"].apply(map_age_group)\n",
    "age_group_mismatch = df[df[\"age_group_2\"] != df[\"expected_age_group\"]]\n",
    "print(\"âŒ Age group khÃ´ng khá»›p vá»›i Age:\")\n",
    "print(age_group_mismatch[[\"id\", \"age\", \"age_group_2\", \"expected_age_group\"]].head(10), \"\\n\")\n",
    "\n",
    "# ğŸ”¹ MPI group khÃ´ng khá»›p vá»›i MPI_mean\n",
    "def map_mpi_group(mpi):\n",
    "    if mpi < 4500: return \"1.Under VND 4.5m\"\n",
    "    elif mpi < 9000: return \"2.VND 4.5m - VND 8.9m\"\n",
    "    elif mpi < 15000: return \"3.VND 9m - VND 14.9m\"\n",
    "    elif mpi < 25000: return \"4.VND 15m - VND 24.9m\"\n",
    "    else: return \"5.VND 25m+\"\n",
    "\n",
    "df[\"expected_mpi_2\"] = df[\"mpi_mean\"].apply(map_mpi_group)\n",
    "mpi_group_mismatch = df[df[\"mpi_2\"] != df[\"expected_mpi_2\"]]\n",
    "print(\"âŒ MPI group khÃ´ng khá»›p vá»›i MPI_Mean:\")\n",
    "print(mpi_group_mismatch[[\"id\", \"mpi_mean\", \"mpi_2\", \"expected_mpi_2\"]].head(10), \"\\n\")\n",
    "\n",
    "# ğŸ”¹ TOM, BUMO, MostFavourite Ä‘á»u trÃ¹ng nhau\n",
    "same_brand_all = df[\n",
    "    (df[\"tom\"].notnull()) & \n",
    "    (df[\"bumo\"].notnull()) & \n",
    "    (df[\"mostfavourite\"].notnull()) & \n",
    "    (df[\"tom\"] == df[\"bumo\"]) & \n",
    "    (df[\"bumo\"] == df[\"mostfavourite\"])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ccd2d6e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\phamh\\AppData\\Local\\Temp\\ipykernel_22736\\1711239559.py:28: DtypeWarning: Columns (8,9,10,14,16) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Táº¥t cáº£ cÃ¡c ID Ä‘á»u cÃ³ máº·t trong SA_var_cleaned.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# ÄÆ°á»ng dáº«n thÆ° má»¥c chá»©a file\n",
    "data_dir = \"cleaned_data/\"\n",
    "\n",
    "# Äá»c ID tá»« báº£ng gá»‘c (báº£ng chÃ­nh)\n",
    "sa = pd.read_csv(os.path.join(data_dir, \"SA_var_cleaned.csv\"))\n",
    "sa.columns = sa.columns.str.strip().str.lower()\n",
    "valid_ids = set(sa[\"id\"].dropna().unique())\n",
    "\n",
    "# Danh sÃ¡ch cÃ¡c file cáº§n kiá»ƒm tra ID\n",
    "files_to_check = [\n",
    "    \"Brand_Health_cleaned.csv\",\n",
    "    \"Brand_Image_cleaned.csv\",\n",
    "    \"Companion_cleaned.csv\",\n",
    "    \"Dayofweek_cleaned.csv\",\n",
    "    \"Daypart_cleaned.csv\",\n",
    "    \"NeedstateDayDaypart_cleaned.csv\"\n",
    "]\n",
    "\n",
    "# LÆ°u káº¿t quáº£ lá»—i\n",
    "missing_id_report = []\n",
    "\n",
    "# Láº·p qua tá»«ng file vÃ  kiá»ƒm tra ID\n",
    "for file_name in files_to_check:\n",
    "    path = os.path.join(data_dir, file_name)\n",
    "    df = pd.read_csv(path)\n",
    "    df.columns = df.columns.str.strip().str.lower()\n",
    "    \n",
    "    if \"id\" in df.columns:\n",
    "        ids = set(df[\"id\"].dropna().unique())\n",
    "        missing_ids = ids - valid_ids\n",
    "        if missing_ids:\n",
    "            missing_id_report.append({\n",
    "                \"file\": file_name,\n",
    "                \"missing_count\": len(missing_ids),\n",
    "                \"sample_missing_ids\": list(missing_ids)[:5]  # in máº«u 5 ID\n",
    "            })\n",
    "\n",
    "# In káº¿t quáº£\n",
    "if missing_id_report:\n",
    "    print(\"âŒ Má»™t sá»‘ file cÃ³ ID khÃ´ng tá»“n táº¡i trong SA_var_cleaned.csv:\")\n",
    "    for report in missing_id_report:\n",
    "        print(f\"ğŸ“ {report['file']}: thiáº¿u {report['missing_count']} ID\")\n",
    "        print(f\"   â†’ VÃ­ dá»¥ ID thiáº¿u: {report['sample_missing_ids']}\")\n",
    "else:\n",
    "    print(\"âœ… Táº¥t cáº£ cÃ¡c ID Ä‘á»u cÃ³ máº·t trong SA_var_cleaned.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a1d51f7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\phamh\\AppData\\Local\\Temp\\ipykernel_22736\\2020792781.py:13: DtypeWarning: Columns (8,9,10,14,16) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ File: Brand_Health_cleaned.csv\n",
      "â¡ï¸ Features: ['id', 'brand', 'spontaneous', 'awareness', 'trial', 'p3m', 'p1m', 'comprehension', 'brand_likability', 'weekly', 'daily', 'fre_visit', 'ppa', 'spending', 'segmentation', 'nps_p3m', 'nps_p3m_group', 'spending_use', 'was_awareness_corrected']\n",
      "--------------------------------------------------------------------------------\n",
      "ğŸ“ File: Brand_Image_cleaned.csv\n",
      "â¡ï¸ Features: ['ID', 'Awareness', 'Attribute', 'BrandImage']\n",
      "--------------------------------------------------------------------------------\n",
      "ğŸ“ File: Companion_cleaned.csv\n",
      "â¡ï¸ Features: ['ID', 'Companion#group']\n",
      "--------------------------------------------------------------------------------\n",
      "ğŸ“ File: Competitor_cleaned.csv\n",
      "â¡ï¸ Features: ['No#', 'Brand', 'City', 'Year', 'StoreCount']\n",
      "--------------------------------------------------------------------------------\n",
      "ğŸ“ File: Dayofweek_cleaned.csv\n",
      "â¡ï¸ Features: ['ID', 'Dayofweek', 'Visit#Dayofweek', 'Weekday#end']\n",
      "--------------------------------------------------------------------------------\n",
      "ğŸ“ File: Daypart_cleaned.csv\n",
      "â¡ï¸ Features: ['ID', 'Daypart', 'Visit#Daypart']\n",
      "--------------------------------------------------------------------------------\n",
      "ğŸ“ File: NeedstateDayDaypart_cleaned.csv\n",
      "â¡ï¸ Features: ['ID', 'Needstates', 'Day#Daypart', 'NeedstateGroup']\n",
      "--------------------------------------------------------------------------------\n",
      "ğŸ“ File: SA_var_cleaned.csv\n",
      "â¡ï¸ Features: ['ID', 'City', 'Group_size', 'Age', 'MPI#Mean', 'TOM', 'BUMO', 'BUMO_Previous', 'MostFavourite', 'Gender', 'Age#Group#2', 'MPI#2', 'Occupation', 'Occupation#group', 'Year']\n",
      "--------------------------------------------------------------------------------\n",
      "ğŸ“ File: Segmentation_cleaned.csv\n",
      "â¡ï¸ Features: ['ID', 'Segmentation', 'Visit', 'Spending', 'Brand', 'PPA']\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# ÄÆ°á»ng dáº«n tá»›i thÆ° má»¥c chá»©a cÃ¡c file Ä‘Ã£ lÃ m sáº¡ch\n",
    "data_dir = \"cleaned_data\"\n",
    "\n",
    "# Duyá»‡t táº¥t cáº£ cÃ¡c file trong thÆ° má»¥c\n",
    "for filename in os.listdir(data_dir):\n",
    "    # Chá»‰ xÃ©t cÃ¡c file CSV (hoáº·c Ä‘á»•i Ä‘iá»u kiá»‡n náº¿u lÃ  Ä‘á»‹nh dáº¡ng khÃ¡c)\n",
    "    if filename.endswith(\".csv\"):\n",
    "        file_path = os.path.join(data_dir, filename)\n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "            print(f\"ğŸ“ File: {filename}\")\n",
    "            print(\"â¡ï¸ Features:\", list(df.columns))\n",
    "            print(\"-\" * 80)\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Lá»—i Ä‘á»c file {filename}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "03444d5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sá»‘ báº£n ghi cÃ³ spending nhÆ°ng thiáº¿u fre_visit: 0\n",
      "Empty DataFrame\n",
      "Columns: [id, spending, fre_visit, ppa]\n",
      "Index: []\n",
      "\n",
      "Sá»‘ ID nÄƒm 2017 nhÆ°ng khÃ´ng cÃ³ trong Segmentation: 9\n",
      "\n",
      "== ThÃ´ng tin trong Brand_Health cá»§a cÃ¡c ID bá»‹ thiáº¿u trong Segmentation ==\n",
      "           id  fre_visit  spending  ppa\n",
      "4779   138658        0.0       0.0  0.0\n",
      "5873   140223        0.0       0.0  0.0\n",
      "5897   141272        0.0       0.0  0.0\n",
      "6130   141270        0.0       0.0  0.0\n",
      "6434   138841        0.0       0.0  0.0\n",
      "6455   140225        0.0       0.0  0.0\n",
      "6797   140345        0.0       0.0  0.0\n",
      "25909  138694        0.0       0.0  0.0\n",
      "26007  140253        0.0       0.0  0.0\n",
      "57134  140253       30.0       0.0  0.0\n",
      "58111  140225        4.0       0.0  0.0\n",
      "58125  141270        4.0       0.0  0.0\n",
      "58240  138841        8.0       0.0  0.0\n",
      "58269  140223        8.0       0.0  0.0\n",
      "58574  141272        4.0       0.0  0.0\n",
      "59323  138658        4.0       0.0  0.0\n",
      "59399  140345        4.0       0.0  0.0\n",
      "60197  138694        4.0       0.0  0.0\n",
      "\n",
      "== Kiá»ƒm tra láº¡i cÃ¡c ID nÃ y trong Segmentation ==\n",
      "Empty DataFrame\n",
      "Columns: [id, segmentation, visit, spending, brand, ppa]\n",
      "Index: []\n",
      "       id  fre_visit  spending  ppa\n",
      "0  138658   0.800000       0.0  0.0\n",
      "1  138694   0.666667       0.0  0.0\n",
      "2  138841   1.333333       0.0  0.0\n",
      "3  140223   1.000000       0.0  0.0\n",
      "4  140225   0.571429       0.0  0.0\n",
      "5  140253   6.000000       0.0  0.0\n",
      "6  140345   0.666667       0.0  0.0\n",
      "7  141270   0.666667       0.0  0.0\n",
      "8  141272   0.363636       0.0  0.0\n",
      "\n",
      "âœ… ÄÃ£ cáº­p nháº­t Segmentation_cleaned.csv vá»›i 9 ID khÃ´ng thá»ƒ phÃ¢n loáº¡i.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\phamh\\AppData\\Local\\Temp\\ipykernel_22736\\111309359.py:5: DtypeWarning: Columns (8,9,10,14,16) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_health = pd.read_csv(\"cleaned_data/Brand_Health_cleaned.csv\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Äá»c dá»¯ liá»‡u\n",
    "df_seg = pd.read_csv(\"cleaned_data/Segmentation_cleaned.csv\")\n",
    "df_health = pd.read_csv(\"cleaned_data/Brand_Health_cleaned.csv\")\n",
    "df_sa = pd.read_csv(\"cleaned_data/SA_var_cleaned.csv\")\n",
    "\n",
    "# Chuáº©n hÃ³a tÃªn cá»™t vá» chá»¯ thÆ°á»ng Ä‘á»ƒ trÃ¡nh lá»—i\n",
    "df_health.columns = df_health.columns.str.lower()\n",
    "df_seg.columns = df_seg.columns.str.lower()\n",
    "df_sa.columns = df_sa.columns.str.lower()\n",
    "\n",
    "# 1. Kiá»ƒm tra báº£n ghi cÃ³ spending khÃ´ng null nhÆ°ng fre_visit bá»‹ thiáº¿u\n",
    "inconsistent_rows = df_health[df_health['spending'].notna() & df_health['fre_visit'].isna()]\n",
    "print(f\"Sá»‘ báº£n ghi cÃ³ spending nhÆ°ng thiáº¿u fre_visit: {len(inconsistent_rows)}\")\n",
    "print(inconsistent_rows[['id', 'spending', 'fre_visit', 'ppa']].head(10))\n",
    "\n",
    "# 2. Lá»c cÃ¡c ID trong SA_var cÃ³ Year = 2017\n",
    "ids_2017 = df_sa[df_sa['year'] == 2017]['id'].unique()\n",
    "\n",
    "# 3. So sÃ¡nh: tÃ¬m cÃ¡c ID nÄƒm 2017 khÃ´ng cÃ³ trong Segmentation\n",
    "missing_ids = set(ids_2017) - set(df_seg['id'])\n",
    "print(f\"\\nSá»‘ ID nÄƒm 2017 nhÆ°ng khÃ´ng cÃ³ trong Segmentation: {len(missing_ids)}\")\n",
    "missing_ids = list(missing_ids)\n",
    "\n",
    "# 4. Kiá»ƒm tra thÃ´ng tin brand health cá»§a cÃ¡c ID bá»‹ thiáº¿u\n",
    "missing_health = df_health[df_health['id'].isin(missing_ids)]\n",
    "print(\"\\n== ThÃ´ng tin trong Brand_Health cá»§a cÃ¡c ID bá»‹ thiáº¿u trong Segmentation ==\")\n",
    "print(missing_health[['id', 'fre_visit', 'spending', 'ppa']].drop_duplicates())\n",
    "\n",
    "# 5. XÃ¡c nháº­n láº¡i cÃ¡c ID nÃ y cÃ³ trong Segmentation khÃ´ng\n",
    "print(\"\\n== Kiá»ƒm tra láº¡i cÃ¡c ID nÃ y trong Segmentation ==\")\n",
    "print(df_seg[df_seg['id'].isin(missing_ids)])\n",
    "\n",
    "# 6. Kiá»ƒm tra dá»¯ liá»‡u hÃ nh vi tá»« Brand_Health cho cÃ¡c ID bá»‹ thiáº¿u\n",
    "# Láº¥y giÃ¡ trá»‹ trung bÃ¬nh (hoáº·c tá»‘i Ä‘a, hoáº·c tá»•ng tuá»³ má»¥c Ä‘Ã­ch)\n",
    "summary = missing_health.groupby('id')[['fre_visit', 'spending', 'ppa']].mean().reset_index()\n",
    "print(summary)\n",
    "# 7. Táº¡o báº£n ghi má»›i cho cÃ¡c ID khÃ´ng thá»ƒ phÃ¢n loáº¡i\n",
    "# GÃ¡n segmentation = 'Not Segmentable', Brand = 0\n",
    "new_rows = pd.DataFrame({\n",
    "    'id': missing_ids,\n",
    "    'segmentation': 'Not Segmentable',\n",
    "    'visit': None,\n",
    "    'spending': None,\n",
    "    'brand': 0,\n",
    "    'ppa': None\n",
    "})\n",
    "\n",
    "# 8. Gá»™p láº¡i vá»›i báº£ng segmentation gá»‘c\n",
    "df_seg_updated = pd.concat([df_seg, new_rows], ignore_index=True)\n",
    "\n",
    "# 9. Xuáº¥t ra file má»›i náº¿u cáº§n\n",
    "df_seg_updated.to_csv(\"cleaned_data/Segmentation_cleaned.csv\", index=False)\n",
    "print(\"\\nâœ… ÄÃ£ cáº­p nháº­t Segmentation_cleaned.csv vá»›i 9 ID khÃ´ng thá»ƒ phÃ¢n loáº¡i.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "31235dd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sá»‘ dÃ²ng inconsistency A1: 0\n",
      "Empty DataFrame\n",
      "Columns: [id, brand, health_ppa, seg_ppa_calc, seg_ppa, abs_diff_ppa]\n",
      "Index: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\phamh\\AppData\\Local\\Temp\\ipykernel_22736\\3033574368.py:6: DtypeWarning: Columns (8,9,10,14,16) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_health = pd.read_csv(\"cleaned_data/Brand_Health_cleaned.csv\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load dá»¯ liá»‡u\n",
    "df_sa = pd.read_csv(\"cleaned_data/SA_var_cleaned.csv\")\n",
    "df_health = pd.read_csv(\"cleaned_data/Brand_Health_cleaned.csv\")\n",
    "df_seg = pd.read_csv(\"cleaned_data/Segmentation_cleaned.csv\")\n",
    "\n",
    "# Chuáº©n hÃ³a tÃªn cá»™t vá» chá»¯ thÆ°á»ng Ä‘á»ƒ trÃ¡nh lá»—i\n",
    "df_sa.columns = df_sa.columns.str.lower()\n",
    "df_health.columns = df_health.columns.str.lower()\n",
    "df_seg.columns = df_seg.columns.str.lower()\n",
    "\n",
    "# Chá»n ID cÃ³ Year = 2017\n",
    "id_2017 = df_sa[df_sa['year'] == 2017]['id'].unique()\n",
    "\n",
    "# Lá»c dá»¯ liá»‡u liÃªn quan\n",
    "df_health_2017 = df_health[df_health['id'].isin(id_2017)].copy()\n",
    "df_seg_2017 = df_seg[df_seg['id'].isin(id_2017)].copy()\n",
    "\n",
    "# Äá»•i tÃªn Ä‘á»ƒ merge vÃ  so sÃ¡nh\n",
    "df_health_2017 = df_health_2017.rename(columns={\n",
    "    'ppa': 'health_ppa',\n",
    "    'spending': 'health_spending',\n",
    "    'fre_visit': 'health_visit'\n",
    "})\n",
    "\n",
    "df_seg_2017 = df_seg_2017.rename(columns={\n",
    "    'ppa': 'seg_ppa',\n",
    "    'spending': 'seg_spending',\n",
    "    'visit': 'seg_visit'\n",
    "})\n",
    "\n",
    "# Merge theo ID vÃ  Brand\n",
    "merged = pd.merge(df_health_2017, df_seg_2017, on=['id', 'brand'], how='inner')\n",
    "\n",
    "# Ã‰p kiá»ƒu sá»‘\n",
    "for col in ['health_ppa', 'health_spending', 'health_visit', 'seg_ppa', 'seg_spending', 'seg_visit']:\n",
    "    merged[col] = pd.to_numeric(merged[col], errors='coerce')\n",
    "\n",
    "# TÃ­nh láº¡i PPA\n",
    "merged['seg_ppa_calc'] = merged['seg_spending'] / merged['seg_visit']\n",
    "\n",
    "# TÃ­nh lá»‡ch tuyá»‡t Ä‘á»‘i vÃ  lá»‡ch tÆ°Æ¡ng Ä‘á»‘i\n",
    "merged['abs_diff_ppa'] = np.abs(merged['health_ppa'] - merged['seg_ppa_calc'])\n",
    "\n",
    "condition = (\n",
    "    (merged['abs_diff_ppa'] > 1) |\n",
    "    ((merged['abs_diff_ppa'] / merged['health_ppa'].replace(0, np.nan)) > 0.2)\n",
    ")\n",
    "\n",
    "# Lá»c inconsistency\n",
    "inconsistencies = merged[condition]\n",
    "\n",
    "print(f\"Sá»‘ dÃ²ng inconsistency A1: {len(inconsistencies)}\")\n",
    "print(inconsistencies[['id', 'brand', 'health_ppa', 'seg_ppa_calc', 'seg_ppa', 'abs_diff_ppa']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "227f6018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C1 â€“ Sá»‘ dÃ²ng segmentation khÃ´ng khá»›p: 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>segmentation_health</th>\n",
       "      <th>segmentation_seg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [id, segmentation_health, segmentation_seg]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_health.columns = df_health.columns.str.lower()\n",
    "df_sa.columns = df_sa.columns.str.lower()\n",
    "df_seg.columns = df_seg.columns.str.lower()\n",
    "\n",
    "# BÃ¢y giá» cá»™t 'ID' trá»Ÿ thÃ nh 'id', 'Segmentation' thÃ nh 'segmentation'\n",
    "df_merge_c1 = df_health[['id', 'segmentation']].merge(\n",
    "    df_sa[['id']], on='id', how='inner'\n",
    ").merge(\n",
    "    df_seg[['id', 'segmentation']], on='id', how='left', suffixes=('_health', '_seg')\n",
    ")\n",
    "\n",
    "# So sÃ¡nh segmentation giá»¯a cÃ¡c báº£ng\n",
    "df_c1_inconsistent = df_merge_c1[\n",
    "    (df_merge_c1['segmentation_health'].notna()) &\n",
    "    (df_merge_c1['segmentation_seg'].notna()) &\n",
    "    (df_merge_c1['segmentation_health'] != df_merge_c1['segmentation_seg'])\n",
    "]\n",
    "\n",
    "print(f\"C1 â€“ Sá»‘ dÃ²ng segmentation khÃ´ng khá»›p: {len(df_c1_inconsistent)}\")\n",
    "display(df_c1_inconsistent[['id', 'segmentation_health', 'segmentation_seg']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b7c81410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C1 â€“ Sá»‘ dÃ²ng segmentation khÃ´ng khá»›p: 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>segmentation_health</th>\n",
       "      <th>segmentation_seg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [id, segmentation_health, segmentation_seg]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C4 â€“ Sá»‘ dÃ²ng cÃ³ group_size < 1 hoáº·c thiáº¿u: 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>group_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [id, group_size]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C5 â€“ Sá»‘ dÃ²ng spending_use > spending: 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>spending</th>\n",
       "      <th>spending_use</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [id, spending, spending_use]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C3 â€“ Sá»‘ dÃ²ng cÃ³ brand khÃ´ng há»£p lá»‡: 42050\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>brand</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>349551</td>\n",
       "      <td>milano</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>349553</td>\n",
       "      <td>milano</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365</th>\n",
       "      <td>349957</td>\n",
       "      <td>milano</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>366</th>\n",
       "      <td>350148</td>\n",
       "      <td>milano</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>367</th>\n",
       "      <td>351103</td>\n",
       "      <td>milano</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74413</th>\n",
       "      <td>456854</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74414</th>\n",
       "      <td>456857</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74415</th>\n",
       "      <td>458063</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74416</th>\n",
       "      <td>458098</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74418</th>\n",
       "      <td>459890</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>38383 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id   brand\n",
       "363    349551  milano\n",
       "364    349553  milano\n",
       "365    349957  milano\n",
       "366    350148  milano\n",
       "367    351103  milano\n",
       "...       ...     ...\n",
       "74413  456854   other\n",
       "74414  456857   other\n",
       "74415  458063   other\n",
       "74416  458098   other\n",
       "74418  459890   other\n",
       "\n",
       "[38383 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CÃ¡c brand khÃ´ng há»£p lá»‡ trong Brand_Health:\n",
      "brand\n",
      "street                    10669\n",
      "other                      9733\n",
      "milano                     4699\n",
      "independent cafe           2717\n",
      "aha cafe                   1852\n",
      "urban station              1667\n",
      "passio                     1627\n",
      "thá»©c coffee                1061\n",
      "viva star                  1029\n",
      "mÃª trang                    836\n",
      "coffee bean & tea leaf      824\n",
      "long cafe                   710\n",
      "gong cha                    594\n",
      "má»™c miÃªn                    585\n",
      "Ä‘en Ä‘Ã¡                      520\n",
      "effoc                       473\n",
      "maxx coffee                 397\n",
      "saigon cafÃ©                 363\n",
      "bonpas                      316\n",
      "runam cafe                  316\n",
      "nia cafe                    263\n",
      "the coffee factory          256\n",
      "koi cafe                    237\n",
      "the cups coffee             133\n",
      "cheese coffee               123\n",
      "laha coffee                  50\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load táº¥t cáº£ datasets\n",
    "df_health = pd.read_csv(\"cleaned_data/Brand_Health_cleaned.csv\", dtype=str, low_memory=False)\n",
    "df_sa = pd.read_csv(\"cleaned_data/SA_var_cleaned.csv\", dtype=str)\n",
    "df_seg = pd.read_csv(\"cleaned_data/Segmentation_cleaned.csv\", dtype=str)\n",
    "df_dow = pd.read_csv(\"cleaned_data/Dayofweek_cleaned.csv\", dtype=str)\n",
    "df_dp = pd.read_csv(\"cleaned_data/Daypart_cleaned.csv\", dtype=str)\n",
    "df_comp = pd.read_csv(\"cleaned_data/Competitor_cleaned.csv\", dtype=str)\n",
    "\n",
    "# Chuáº©n hÃ³a tÃªn cá»™t thÃ nh chá»¯ thÆ°á»ng Ä‘á»ƒ trÃ¡nh lá»—i vá» KeyError\n",
    "df_health.columns = df_health.columns.str.lower()\n",
    "df_sa.columns = df_sa.columns.str.lower()\n",
    "df_seg.columns = df_seg.columns.str.lower()\n",
    "df_dow.columns = df_dow.columns.str.lower()\n",
    "df_dp.columns = df_dp.columns.str.lower()\n",
    "df_comp.columns = df_comp.columns.str.lower()\n",
    "\n",
    "# Chuyá»ƒn cÃ¡c cá»™t cáº§n thiáº¿t sang kiá»ƒu sá»‘\n",
    "df_health['fre_visit'] = pd.to_numeric(df_health['fre_visit'], errors='coerce')\n",
    "df_health['spending'] = pd.to_numeric(df_health['spending'], errors='coerce')\n",
    "df_health['spending_use'] = pd.to_numeric(df_health['spending_use'], errors='coerce')\n",
    "df_dow['visit#dayofweek'] = pd.to_numeric(df_dow['visit#dayofweek'], errors='coerce')\n",
    "df_dp['visit#daypart'] = pd.to_numeric(df_dp['visit#daypart'], errors='coerce')\n",
    "df_sa['group_size'] = pd.to_numeric(df_sa['group_size'], errors='coerce')\n",
    "\n",
    "# =============================\n",
    "# âœ… C1. Kiá»ƒm tra sá»± khÃ´ng khá»›p cá»§a segmentation giá»¯a cÃ¡c báº£ng\n",
    "# =============================\n",
    "# GhÃ©p df_health (chá»‰ láº¥y id, segmentation) vá»›i df_sa (chá»‰ láº¥y id) rá»“i merge vá»›i df_seg (id, segmentation)\n",
    "df_merge_c1 = df_health[['id', 'segmentation']].merge(\n",
    "    df_sa[['id']], on='id', how='inner'\n",
    ").merge(\n",
    "    df_seg[['id', 'segmentation']], on='id', how='left', suffixes=('_health', '_seg')\n",
    ")\n",
    "\n",
    "# Lá»c cÃ¡c dÃ²ng mÃ  segmentation khÃ¡c nhau (vá»›i cáº£ 2 báº£ng Ä‘á»u khÃ¡c NaN)\n",
    "df_c1_inconsistent = df_merge_c1[\n",
    "    (df_merge_c1['segmentation_health'].notna()) &\n",
    "    (df_merge_c1['segmentation_seg'].notna()) &\n",
    "    (df_merge_c1['segmentation_health'] != df_merge_c1['segmentation_seg'])\n",
    "]\n",
    "\n",
    "print(f\"C1 â€“ Sá»‘ dÃ²ng segmentation khÃ´ng khá»›p: {len(df_c1_inconsistent)}\")\n",
    "display(df_c1_inconsistent[['id', 'segmentation_health', 'segmentation_seg']])\n",
    "\n",
    "# =============================\n",
    "# âœ… C4. Kiá»ƒm tra group_size pháº£i >= 1\n",
    "# =============================\n",
    "df_c4_invalid = df_sa[(df_sa['group_size'] < 1) | (df_sa['group_size'].isna())]\n",
    "print(f\"C4 â€“ Sá»‘ dÃ²ng cÃ³ group_size < 1 hoáº·c thiáº¿u: {len(df_c4_invalid)}\")\n",
    "display(df_c4_invalid[['id', 'group_size']])\n",
    "\n",
    "# =============================\n",
    "# âœ… C5. Kiá»ƒm tra spending_use â‰¤ spending\n",
    "# =============================\n",
    "df_c5_invalid = df_health[\n",
    "    (df_health['spending'].notna()) &\n",
    "    (df_health['spending_use'].notna()) &\n",
    "    (df_health['spending_use'] > df_health['spending'])\n",
    "]\n",
    "print(f\"C5 â€“ Sá»‘ dÃ²ng spending_use > spending: {len(df_c5_invalid)}\")\n",
    "display(df_c5_invalid[['id', 'spending', 'spending_use']])\n",
    "\n",
    "# =============================\n",
    "# âœ… C3. Kiá»ƒm tra brand pháº£i náº±m trong danh sÃ¡ch brand há»£p lá»‡ (danh sÃ¡ch láº¥y tá»« Competitor)\n",
    "# =============================\n",
    "# Chuáº©n hÃ³a cá»™t brand cho cáº£ df_health vÃ  df_comp\n",
    "df_health['brand'] = df_health['brand'].str.lower().str.strip()\n",
    "df_comp['brand'] = df_comp['brand'].str.lower().str.strip()\n",
    "\n",
    "# Láº¥y danh sÃ¡ch cÃ¡c brand há»£p lá»‡\n",
    "valid_brands = df_comp['brand'].dropna().unique()\n",
    "\n",
    "# Lá»c cÃ¡c dÃ²ng trong Brand_Health cÃ³ brand khÃ´ng thuá»™c danh sÃ¡ch há»£p lá»‡\n",
    "df_c3_invalid = df_health[~df_health['brand'].isin(valid_brands)]\n",
    "print(f\"C3 â€“ Sá»‘ dÃ²ng cÃ³ brand khÃ´ng há»£p lá»‡: {len(df_c3_invalid)}\")\n",
    "display(df_c3_invalid[['id', 'brand']].drop_duplicates())\n",
    "print(\"CÃ¡c brand khÃ´ng há»£p lá»‡ trong Brand_Health:\")\n",
    "print(df_c3_invalid['brand'].value_counts())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
