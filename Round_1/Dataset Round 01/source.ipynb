{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a227374c",
   "metadata": {},
   "source": [
    "Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8dd09d9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmentation\n",
      "Brand_Image\n",
      "Brand_Health\n",
      "Companion\n",
      "Competitor\n",
      "Dayofweek\n",
      "Daypart\n",
      "Needstate\n",
      "SA_var\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. Segmentation\n",
    "print(\"Segmentation\")\n",
    "df_seg = pd.read_csv('Data/2017Segmentation3685case.csv', sep=';', on_bad_lines='skip')\n",
    "df_seg.columns = df_seg.columns.str.strip()\n",
    "\n",
    "# 2. Brand_Image\n",
    "print(\"Brand_Image\")\n",
    "df_image = pd.read_csv('Data/Brand_Image.csv', sep=';', on_bad_lines='skip')\n",
    "df_image.columns = df_image.columns.str.strip()\n",
    "\n",
    "# 3. Brand_Health\n",
    "print(\"Brand_Health\")\n",
    "df_health = pd.read_csv(\n",
    "    'Data/Brandhealth.csv',\n",
    "    sep=';',\n",
    "    on_bad_lines='skip',\n",
    "    low_memory=False\n",
    ")\n",
    "df_health.columns = df_health.columns.str.strip()\n",
    "\n",
    "# 4. Companion\n",
    "print(\"Companion\")\n",
    "df_companion = pd.read_csv('Data/Companion.csv', sep=';', on_bad_lines='skip')\n",
    "df_companion.columns = df_companion.columns.str.strip()\n",
    "\n",
    "# 5. Competitor\n",
    "print(\"Competitor\")\n",
    "df_competitor = pd.read_csv('Data/Competitor database_xlnm#_FilterDatabase.csv', sep=';', on_bad_lines='skip')\n",
    "df_competitor.columns = df_competitor.columns.str.strip()\n",
    "\n",
    "# 6. Dayofweek\n",
    "print(\"Dayofweek\")\n",
    "df_dow = pd.read_csv('Data/Dayofweek.csv', sep=';', on_bad_lines='skip')\n",
    "df_dow.columns = df_dow.columns.str.strip()\n",
    "\n",
    "# 7. Daypart\n",
    "print(\"Daypart\")\n",
    "df_daypart = pd.read_csv('Data/Daypart.csv', sep=';', on_bad_lines='skip')\n",
    "df_daypart.columns = df_daypart.columns.str.strip()\n",
    "\n",
    "# 8. Needstate\n",
    "print(\"Needstate\")\n",
    "df_need = pd.read_csv('Data/NeedstateDayDaypart.csv', sep=';', on_bad_lines='skip')\n",
    "df_need.columns = df_need.columns.str.strip()\n",
    "\n",
    "# 9. SA_var\n",
    "print(\"SA_var\")\n",
    "df_sa = pd.read_csv('Data/SA#var.csv', sep=';', on_bad_lines='skip')\n",
    "df_sa.columns = df_sa.columns.str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4017456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Brand_Image is consistent with SA#var\n",
      "✅ Brand_Health is consistent with SA#var\n",
      "✅ Companion is consistent with SA#var\n",
      "✅ Dayofweek is consistent with SA#var\n",
      "✅ Daypart is consistent with SA#var\n",
      "✅ NeedstateDayDaypart is consistent with SA#var\n"
     ]
    }
   ],
   "source": [
    "# 🔁 Tạo dict mapping ID -> (City, Year) từ df_sa\n",
    "id_map = df_sa.set_index('ID')[['City', 'Year']].dropna().astype(str)\n",
    "\n",
    "# 🧠 Hàm kiểm tra từng dataframe\n",
    "def check_consistency(df, name):\n",
    "    df = df[['ID', 'City', 'Year']].dropna().astype(str)\n",
    "    df = df[df['ID'].isin(id_map.index)]\n",
    "    merged = df.merge(id_map, on='ID', suffixes=('', '_sa'))\n",
    "    mismatch = merged[(merged['City'] != merged['City_sa']) | (merged['Year'] != merged['Year_sa'])]\n",
    "    \n",
    "    if not mismatch.empty:\n",
    "        print(f\"❌ Inconsistent City/Year found in {name}: {len(mismatch)} mismatches\")\n",
    "        display(mismatch.head())\n",
    "    else:\n",
    "        print(f\"✅ {name} is consistent with SA#var\")\n",
    "\n",
    "# 📋 Danh sách sheet cần kiểm\n",
    "sheets = {\n",
    "    'Brand_Image': df_image,\n",
    "    'Brand_Health': df_health,\n",
    "    'Companion': df_companion,\n",
    "    'Dayofweek': df_dow,\n",
    "    'Daypart': df_daypart,\n",
    "    'NeedstateDayDaypart': df_need,\n",
    "}\n",
    "\n",
    "# 🔍 Chạy kiểm tra\n",
    "for name, df in sheets.items():\n",
    "    check_consistency(df, name)\n",
    "def clean_brand_columns(df, columns, replace_map, name=\"DataFrame\"):\n",
    "    \"\"\"\n",
    "    Chuẩn hoá các cột thương hiệu trong DataFrame:\n",
    "    - Viết thường\n",
    "    - Xoá khoảng trắng\n",
    "    - Thay thế các biến thể thương hiệu theo bản đồ thay thế\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame cần xử lý\n",
    "        columns (list): Danh sách tên cột cần xử lý\n",
    "        replace_map (dict): Từ điển ánh xạ giá trị cần thay thế\n",
    "        name (str): Tên bảng để in log\n",
    "    \"\"\"\n",
    "    for col in columns:\n",
    "        if col not in df.columns:\n",
    "            print(f\"⚠️  Cột '{col}' không tồn tại trong {name}\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\n🔎 Unique values in '{col}' BEFORE cleaning ({name}):\")\n",
    "        print(df[col].dropna().unique())\n",
    "\n",
    "        df[col] = df[col].str.lower().str.strip()\n",
    "        df[col] = df[col].replace(replace_map)\n",
    "\n",
    "        print(f\"✅ Unique values in '{col}' AFTER cleaning ({name}):\")\n",
    "        print(df[col].dropna().unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b21eaa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📄 Đang xử lý: Segmentation\n",
      "🔍 Số dòng trùng lặp: 0\n",
      "💾 Đã lưu: Segmentation_cleaned.csv\n",
      "\n",
      "📄 Đang xử lý: Brand_Image\n",
      "🔍 Số dòng trùng lặp: 13974\n",
      "✅ Đã loại bỏ 13974 dòng trùng lặp\n",
      "🗑 Đã loại cột: City\n",
      "🗑 Đã loại cột: Year\n",
      "🔧 Đã chuẩn hoá cột Attribute\n",
      "🔧 Đã chuẩn hoá cột Awareness\n",
      "🔧 Đã chuẩn hoá cột BrandImage\n",
      "💾 Đã lưu: Brand_Image_cleaned.csv\n",
      "\n",
      "📄 Đang xử lý: Brand_Health\n",
      "🔍 Số dòng trùng lặp: 0\n",
      "🗑 Đã loại cột: City\n",
      "🗑 Đã loại cột: Year\n",
      "🔧 Chuẩn hoá cột Brand trong bảng Brand_Health\n",
      "🔧 Chuẩn hoá cột Awareness trong bảng Brand_Health\n",
      "🔧 Chuẩn hoá cột Spontaneous trong bảng Brand_Health\n",
      "🔧 Chuẩn hoá cột Trial trong bảng Brand_Health\n",
      "🔧 Chuẩn hoá cột P3M trong bảng Brand_Health\n",
      "🔧 Chuẩn hoá cột P1M trong bảng Brand_Health\n",
      "🔧 Chuẩn hoá cột Weekly trong bảng Brand_Health\n",
      "🔧 Chuẩn hoá cột Daily trong bảng Brand_Health\n",
      "💾 Đã lưu: Brand_Health_cleaned.csv\n",
      "\n",
      "📄 Đang xử lý: Companion\n",
      "🔍 Số dòng trùng lặp: 799\n",
      "✅ Đã loại bỏ 799 dòng trùng lặp\n",
      "🗑 Đã loại cột: City\n",
      "🗑 Đã loại cột: Year\n",
      "💾 Đã lưu: Companion_cleaned.csv\n",
      "\n",
      "📄 Đang xử lý: Competitor\n",
      "🔍 Số dòng trùng lặp: 0\n",
      "💾 Đã lưu: Competitor_cleaned.csv\n",
      "\n",
      "📄 Đang xử lý: Dayofweek\n",
      "🔍 Số dòng trùng lặp: 37\n",
      "✅ Đã loại bỏ 37 dòng trùng lặp\n",
      "🗑 Đã loại cột: City\n",
      "🗑 Đã loại cột: Year\n",
      "💾 Đã lưu: Dayofweek_cleaned.csv\n",
      "\n",
      "📄 Đang xử lý: Daypart\n",
      "🔍 Số dòng trùng lặp: 0\n",
      "🗑 Đã loại cột: City\n",
      "🗑 Đã loại cột: Year\n",
      "💾 Đã lưu: Daypart_cleaned.csv\n",
      "\n",
      "📄 Đang xử lý: NeedstateDayDaypart\n",
      "🔍 Số dòng trùng lặp: 72\n",
      "✅ Đã loại bỏ 72 dòng trùng lặp\n",
      "🗑 Đã loại cột: City\n",
      "🗑 Đã loại cột: Year\n",
      "💾 Đã lưu: NeedstateDayDaypart_cleaned.csv\n",
      "\n",
      "📄 Đang xử lý: SA_var\n",
      "🔍 Số dòng trùng lặp: 0\n",
      "🔧 Chuẩn hoá cột TOM trong bảng SA_var\n",
      "🔧 Chuẩn hoá cột BUMO trong bảng SA_var\n",
      "🔧 Chuẩn hoá cột MostFavourite trong bảng SA_var\n",
      "🗑 Đã loại các cột dư thừa đặc biệt\n",
      "💾 Đã lưu: SA_var_cleaned.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# 📂 Tạo thư mục output nếu chưa tồn tại\n",
    "output_dir = \"./cleaned_data/\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# 📌 Map chuẩn hóa brand để dùng chung\n",
    "brand_replace_map = {\n",
    "    'street / half street coffee (including carts)': 'street',\n",
    "    'Street / Half street coffee (including carts)': 'street',\n",
    "    'other branded cafe chain': 'other',\n",
    "    'other 1': 'other',\n",
    "    'other 2': 'other',\n",
    "    'other 3': 'other',\n",
    "    'indepedent cafe': 'independent cafe'\n",
    "}\n",
    "\n",
    "# 🧹 Hàm chuẩn hoá cột brand\n",
    "def clean_brand_columns(df, columns, replace_map, sheet_name):\n",
    "    for col in columns:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].str.lower().str.strip()\n",
    "            df[col] = df[col].replace(replace_map)\n",
    "            print(f\"🔧 Chuẩn hoá cột {col} trong bảng {sheet_name}\")\n",
    "\n",
    "# 🧠 Hàm tách ngày và thời gian trong NeedstateDayDaypart\n",
    "def clean_day_daypart(val):\n",
    "    val = str(val).strip().lower()\n",
    "    if val in ['overall', 'weekdays', 'weekends']:\n",
    "        return pd.Series({'DayGroup': val, 'Daypart_cleaned': None})\n",
    "    else:\n",
    "        return pd.Series({'DayGroup': None, 'Daypart_cleaned': val})\n",
    "\n",
    "# 🧠 Hàm chuẩn hoá needstates\n",
    "def standardize_needstate(val):\n",
    "    val = str(val).strip().lower()\n",
    "    typo_map = {\n",
    "        'socialzing': 'socializing with others',\n",
    "        'enterntainment (watching movies. playing games, browsing web,…)': 'entertainment',\n",
    "        'enterntainment': 'entertainment'\n",
    "    }\n",
    "    val = typo_map.get(val, val)\n",
    "    needstate_map = {\n",
    "        'socializing with colleagues': 'socializing with others',\n",
    "        'socializing with family / relatives': 'socializing with others',\n",
    "        'socializing with friends': 'socializing with others',\n",
    "        'drinking coffee': 'drinking',\n",
    "        'drinking tea': 'drinking',\n",
    "        'drinking ice-blended': 'drinking',\n",
    "        'drinking other beverages (excluding tea, coffee, freeze)': 'drinking',\n",
    "        'entertainment': 'entertainment',\n",
    "        'relaxing (alone)': 'relaxing',\n",
    "        'have meals (breakfast / lunch / dinner)': 'eating',\n",
    "        'have snack / pastry': 'eating',\n",
    "        'working / business meeting': 'working',\n",
    "        'studying / reading books': 'studying'\n",
    "    }\n",
    "    return needstate_map.get(val, val)\n",
    "\n",
    "# 📦 Hàm xử lý tất cả các bảng\n",
    "def process_df(df, name, drop_cols=['City', 'Year']):\n",
    "    print(f\"\\n📄 Đang xử lý: {name}\")\n",
    "    print(f\"🔍 Số dòng trùng lặp: {df.duplicated().sum()}\")\n",
    "\n",
    "    df_clean = df.drop_duplicates()\n",
    "\n",
    "    if df.shape[0] != df_clean.shape[0]:\n",
    "        print(f\"✅ Đã loại bỏ {df.shape[0] - df_clean.shape[0]} dòng trùng lặp\")\n",
    "\n",
    "    for col in drop_cols:\n",
    "        if col in df_clean.columns:\n",
    "            df_clean = df_clean.drop(columns=col)\n",
    "            print(f\"🗑 Đã loại cột: {col}\")\n",
    "\n",
    "    if name == \"SA_var\":\n",
    "        cols_to_drop = ['MPI_Mean_Use','MPI#detail','MPI','Age#group','Col']\n",
    "        df_clean = df_clean.drop(columns=[c for c in cols_to_drop if c in df_clean.columns])\n",
    "        brand_columns = ['TOM','BUMO','MostFavourite']\n",
    "        clean_brand_columns(df_clean, brand_columns, brand_replace_map, name)\n",
    "        print(\"🗑 Đã loại các cột dư thừa đặc biệt\")\n",
    "\n",
    "    elif name == \"Brand_Image\":\n",
    "        if 'Attribute' in df_clean.columns:\n",
    "            df_clean['Attribute'] = df_clean['Attribute'].str.lower().str.strip()\n",
    "            df_clean['Attribute'] = df_clean['Attribute'].replace({\n",
    "                'good place for working / business meeting': 'good place for working',\n",
    "                'good place for working / studying': 'good place for working',\n",
    "                'good place for socializing with friends': 'good place for socializing',\n",
    "                'good place for socializing with family': 'good place for socializing',\n",
    "                'good place for socializing with colleagues': 'good place for socializing',\n",
    "                'nice environment design': 'pleasant environment',\n",
    "                'comfortable and relaxing environment': 'pleasant environment',\n",
    "                'good other beverages (other than coffee, tea & ice-blended)': 'good other beverages',\n",
    "                'have new product regularly': 'frequent product updates',\n",
    "                'feel i belong here': 'sense of belonging',\n",
    "                'delicious food': 'good food taste'\n",
    "            })\n",
    "            print(\"🔧 Đã chuẩn hoá cột Attribute\")\n",
    "\n",
    "        for col in ['Awareness', 'BrandImage']:\n",
    "            if col in df_clean.columns:\n",
    "                df_clean[col] = df_clean[col].str.lower().str.strip()\n",
    "                df_clean[col] = df_clean[col].replace(brand_replace_map)\n",
    "                print(f\"🔧 Đã chuẩn hoá cột {col}\")\n",
    "\n",
    "    elif name == \"Brand_Health\":\n",
    "        brand_columns = ['Brand', 'Awareness', 'Spontaneous', 'Trial', 'P3M', 'P1M', 'Weekly', 'Daily']\n",
    "        clean_brand_columns(df_clean, brand_columns, brand_replace_map, name)\n",
    "\n",
    "    # elif name == \"NeedstateDayDaypart\":\n",
    "    #     if 'Day#Daypart' in df_clean.columns:\n",
    "    #         df_daypart_split = df_clean['Day#Daypart'].apply(clean_day_daypart).apply(pd.Series)\n",
    "    #         df_clean = pd.concat([df_clean, df_daypart_split], axis=1)\n",
    "    #         df_clean.drop(columns=['Day#Daypart'], inplace=True)\n",
    "    #         print(\"🔧 Đã tách cột Day#Daypart thành DayGroup & Daypart_cleaned\")\n",
    "\n",
    "    #     if 'Needstates' in df_clean.columns:\n",
    "    #         df_clean['Needstates_standardized'] = df_clean['Needstates'].apply(standardize_needstate)\n",
    "    #         print(\"🔧 Đã chuẩn hoá cột Needstates\")\n",
    "            \n",
    "    #     df_clean = df_clean.drop(columns=['Needstates'])\n",
    "\n",
    "    # 💾 Ghi file ra\n",
    "    output_path = os.path.join(output_dir, f\"{name}_cleaned.csv\")\n",
    "    df_clean.to_csv(output_path, index=False)\n",
    "    print(f\"💾 Đã lưu: {name}_cleaned.csv\")\n",
    "\n",
    "# ✨ Gọi xử lý cho từng file\n",
    "process_df(df_seg, \"Segmentation\", drop_cols=[])  \n",
    "process_df(df_image, \"Brand_Image\")\n",
    "process_df(df_health, \"Brand_Health\")\n",
    "process_df(df_companion, \"Companion\")\n",
    "process_df(df_competitor, \"Competitor\", drop_cols=[])  \n",
    "process_df(df_dow, \"Dayofweek\")\n",
    "process_df(df_daypart, \"Daypart\")\n",
    "process_df(df_need, \"NeedstateDayDaypart\")\n",
    "process_df(df_sa, \"SA_var\", drop_cols=[])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0bb2aa8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📁 File: Brand_Health_cleaned.csv\n",
      "📏 Tổng số dòng: 74419\n",
      "          Column  Null Count  Null %\n",
      "     Spontaneous       43426   58.35\n",
      "       Awareness         114    0.15\n",
      "           Trial       27089   36.40\n",
      "             P3M       45570   61.23\n",
      "             P1M       55020   73.93\n",
      "   Comprehension       48073   64.60\n",
      "Brand_Likability       64088   86.12\n",
      "          Weekly       61037   82.02\n",
      "           Daily       66798   89.76\n",
      "       Fre#visit       55087   74.02\n",
      "             PPA       60346   81.09\n",
      "        Spending       60346   81.09\n",
      "    Segmentation       60346   81.09\n",
      "         NPS#P3M       52814   70.97\n",
      "   NPS#P3M#Group       52814   70.97\n",
      "    Spending_use       60346   81.09\n",
      "\n",
      "📁 File: Brand_Image_cleaned.csv\n",
      "📏 Tổng số dòng: 629098\n",
      "   Column  Null Count  Null %\n",
      "Awareness         397    0.06\n",
      "\n",
      "✅ File: Companion_cleaned.csv - KHÔNG có giá trị null\n",
      "📏 Tổng số dòng: 19940\n",
      "\n",
      "✅ File: Competitor_cleaned.csv - KHÔNG có giá trị null\n",
      "📏 Tổng số dòng: 234\n",
      "\n",
      "📁 File: Dayofweek_cleaned.csv\n",
      "📏 Tổng số dòng: 39058\n",
      "         Column  Null Count  Null %\n",
      "      Dayofweek          49    0.13\n",
      "Visit#Dayofweek          54    0.14\n",
      "    Weekday#end          49    0.13\n",
      "\n",
      "📁 File: Daypart_cleaned.csv\n",
      "📏 Tổng số dòng: 19189\n",
      "       Column  Null Count  Null %\n",
      "      Daypart          13    0.07\n",
      "Visit#Daypart         847    4.41\n",
      "\n",
      "✅ File: NeedstateDayDaypart_cleaned.csv - KHÔNG có giá trị null\n",
      "📏 Tổng số dòng: 75179\n",
      "\n",
      "📁 File: SA_var_cleaned.csv\n",
      "📏 Tổng số dòng: 11761\n",
      "       Column  Null Count  Null %\n",
      "   Group_size          15    0.13\n",
      "          Age           9    0.08\n",
      "     MPI#Mean        3717   31.60\n",
      "BUMO_Previous        5665   48.17\n",
      "  Age#Group#2           9    0.08\n",
      "        MPI#2        3717   31.60\n",
      "\n",
      "✅ File: Segmentation_cleaned.csv - KHÔNG có giá trị null\n",
      "📏 Tổng số dòng: 4944\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# 📂 Thư mục chứa file đã xử lý\n",
    "cleaned_dir = \"./cleaned_data/\"\n",
    "\n",
    "# 📊 Hàm kiểm tra null cho từng cột và in tổng số dòng\n",
    "def report_nulls(file_path):\n",
    "    df = pd.read_csv(file_path, sep=',', low_memory=False)\n",
    "    total_rows = df.shape[0]\n",
    "    null_info = df.isnull().sum()\n",
    "    null_percent = (null_info / total_rows * 100).round(2)\n",
    "    \n",
    "    null_df = pd.DataFrame({\n",
    "        \"Column\": null_info.index,\n",
    "        \"Null Count\": null_info.values,\n",
    "        \"Null %\": null_percent.values\n",
    "    })\n",
    "    \n",
    "    null_df = null_df[null_df[\"Null Count\"] > 0]\n",
    "\n",
    "    file_name = os.path.basename(file_path)\n",
    "    \n",
    "    if not null_df.empty:\n",
    "        print(f\"\\n📁 File: {file_name}\")\n",
    "        print(f\"📏 Tổng số dòng: {total_rows}\")\n",
    "        print(null_df.to_string(index=False))\n",
    "    else:\n",
    "        print(f\"\\n✅ File: {file_name} - KHÔNG có giá trị null\")\n",
    "        print(f\"📏 Tổng số dòng: {total_rows}\")\n",
    "\n",
    "# 🚀 Quét tất cả file CSV đã xử lý\n",
    "for fname in os.listdir(cleaned_dir):\n",
    "    if fname.endswith(\".csv\"):\n",
    "        report_nulls(os.path.join(cleaned_dir, fname))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9033a91a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📏 Số dòng ban đầu: 39058\n",
      "✅ Số dòng sau khi loại bỏ dòng null: 38955\n",
      "🗑️ Đã loại bỏ 36296 dòng chứa giá trị null\n",
      "✅ Đã xử lý và lưu lại tại: cleaned_data/SA_var_cleaned.csv\n",
      "📁 File: Brand_Image_cleaned.csv\n",
      "📏 Tổng số dòng: 629098\n",
      "   Column  Null Count  Null %\n",
      "Awareness 397    0.06%\n",
      "✅ Đã xử lý và lưu lại tại: ./cleaned_data/Brand_Image_cleaned.csv\n",
      "\n",
      "📁 File: Daypart_cleaned.csv\n",
      "📏 Tổng số dòng: 19189\n",
      "🧭 Daypart missing: 13 (0.07%)\n",
      "🧭 Visit#Daypart missing: 847 (4.41%)\n",
      "✅ Đã xử lý và lưu lại tại: ./cleaned_data/Daypart_cleaned.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\phamh\\AppData\\Local\\Temp\\ipykernel_22736\\481900124.py:122: DtypeWarning: Columns (2,5,6,8,9,10,14,16) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📁 File: Brand_Health_cleaned.csv\n",
      "📏 Tổng số dòng: 74419\n",
      "🧭 Spontaneous missing: 43426 (58.35%)\n",
      "🧭 Awareness missing: 114 (0.15%)\n",
      "✅ Awareness sau xử lý: còn 0 dòng thiếu.\n",
      "✅ Đã xử lý và lưu lại tại: ./cleaned_data/Brand_Health_cleaned.csv\n",
      "✅ Sau khi ghi đè theo logic:\n",
      "Trial = 1: 47337 / 74419\n",
      "P3M   = 1: 28852 / 74419\n",
      "P1M   = 1: 19399 / 74419\n",
      "💾 Đã ghi đè file: Brand_Health_cleaned.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\phamh\\AppData\\Local\\Temp\\ipykernel_22736\\481900124.py:181: DtypeWarning: Columns (8,9,10,14,16) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(\"cleaned_data/Brand_Health_cleaned.csv\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Đã xử lý thành công!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 🔹 Bước 1: Làm sạch Dayofweek_cleaned.csv\n",
    "df_day = pd.read_csv(\"./cleaned_data/Dayofweek_cleaned.csv\")\n",
    "print(f\"📏 Số dòng ban đầu: {df_day.shape[0]}\")\n",
    "\n",
    "df_day = df_day.dropna()\n",
    "\n",
    "print(f\"✅ Số dòng sau khi loại bỏ dòng null: {df_day.shape[0]}\")\n",
    "print(f\"🗑️ Đã loại bỏ {df.shape[0] - df_day.shape[0]} dòng chứa giá trị null\")\n",
    "\n",
    "# Ghi đè lại file\n",
    "df_day.to_csv(\"./cleaned_data/Dayofweek_cleaned.csv\", index=False)\n",
    "\n",
    "\n",
    "# 🔹 Bước 2: Làm sạch SA_var_cleaned.csv\n",
    "df = pd.read_csv(\"cleaned_data/SA_var_cleaned.csv\")\n",
    "\n",
    "# Điền thiếu Group_size và Age bằng median\n",
    "df[\"Group_size\"] = df[\"Group_size\"].fillna(df[\"Group_size\"].median())\n",
    "df[\"Age\"] = df[\"Age\"].fillna(df[\"Age\"].median())\n",
    "\n",
    "# Tạo nhóm tuổi từ Age\n",
    "def map_age_group(age):\n",
    "    if age <= 19:\n",
    "        return '16 - 19 y.o.'\n",
    "    elif age <= 24:\n",
    "        return '20 - 24 y.o.'\n",
    "    elif age <= 29:\n",
    "        return '25 - 29 y.o.'\n",
    "    elif age <= 34:\n",
    "        return '30 - 34 y.o.'\n",
    "    elif age <= 39:\n",
    "        return '35 - 39 y.o.'\n",
    "    elif age <= 44:\n",
    "        return '40 - 44 y.o.'\n",
    "    else:\n",
    "        return '45+ y.o.'\n",
    "\n",
    "df[\"Age#Group#2\"] = df[\"Age\"].apply(map_age_group)\n",
    "\n",
    "# Điền thiếu MPI#Mean theo nhóm tuổi, rồi toàn bộ\n",
    "df[\"MPI#Mean\"] = df.groupby(\"Age#Group#2\")[\"MPI#Mean\"].transform(lambda x: x.fillna(x.median()))\n",
    "df[\"MPI#Mean\"] = df[\"MPI#Mean\"].fillna(df[\"MPI#Mean\"].median())\n",
    "\n",
    "# Suy ra MPI#2 từ MPI#Mean\n",
    "def map_mpi_category(mpi):\n",
    "    if mpi < 4500:\n",
    "        return \"1.Under VND 4.5m\"\n",
    "    elif mpi < 9000:\n",
    "        return \"2.VND 4.5m - VND 8.9m\"\n",
    "    elif mpi < 15000:\n",
    "        return \"3.VND 9m - VND 14.9m\"\n",
    "    elif mpi < 25000:\n",
    "        return \"4.VND 15m - VND 24.9m\"\n",
    "    else:\n",
    "        return \"5.VND 25m+\"\n",
    "\n",
    "df[\"MPI#2\"] = df[\"MPI#Mean\"].apply(map_mpi_category)\n",
    "\n",
    "# Thay null ở cột BUMO_Previous bằng \"No\"\n",
    "df[\"BUMO_Previous\"] = df[\"BUMO_Previous\"].fillna(\"No\")\n",
    "\n",
    "# Ghi đè lại file\n",
    "df.to_csv(\"cleaned_data/SA_var_cleaned.csv\", index=False)\n",
    "print(\"✅ Đã xử lý và lưu lại tại: cleaned_data/SA_var_cleaned.csv\")\n",
    "\n",
    "\n",
    "# 🔹 Bước 3: Làm sạch Brand_Image_cleaned.csv với xử lý hợp lý\n",
    "file_path = \"./cleaned_data/Brand_Image_cleaned.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# In thống kê ban đầu\n",
    "total_rows = df.shape[0]\n",
    "null_count = df[\"Awareness\"].isnull().sum()\n",
    "print(f\"📁 File: Brand_Image_cleaned.csv\")\n",
    "print(f\"📏 Tổng số dòng: {total_rows}\")\n",
    "print(f\"   Column  Null Count  Null %\")\n",
    "print(f\"Awareness {null_count}    {round(100 * null_count / total_rows, 2)}%\")\n",
    "\n",
    "# Gán cẩn thận: nếu Awareness null mà BrandImage có giá trị → suy luận từ BrandImage\n",
    "df.loc[df[\"Awareness\"].isnull() & df[\"BrandImage\"].notnull(), \"Awareness\"] = df[\"BrandImage\"]\n",
    "\n",
    "# Nếu vẫn còn null, tức là thực sự \"Unaware\"\n",
    "df[\"Awareness\"] = df[\"Awareness\"].fillna(\"Unaware\")\n",
    "\n",
    "# Ghi đè lại file\n",
    "df.to_csv(file_path, index=False)\n",
    "print(f\"✅ Đã xử lý và lưu lại tại: {file_path}\")\n",
    "# ──────────────────────────────────────────────\n",
    "# 🔹 Bước 4: Làm sạch Daypart_cleaned.csv\n",
    "# ──────────────────────────────────────────────\n",
    "file_path = \"./cleaned_data/Daypart_cleaned.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "print(\"\\n📁 File: Daypart_cleaned.csv\")\n",
    "print(f\"📏 Tổng số dòng: {df.shape[0]}\")\n",
    "\n",
    "# Thống kê null\n",
    "daypart_null = df[\"Daypart\"].isnull().sum()\n",
    "visit_null = df[\"Visit#Daypart\"].isnull().sum()\n",
    "print(f\"🧭 Daypart missing: {daypart_null} ({round(100 * daypart_null / df.shape[0], 2)}%)\")\n",
    "print(f\"🧭 Visit#Daypart missing: {visit_null} ({round(100 * visit_null / df.shape[0], 2)}%)\")\n",
    "\n",
    "# 🔸 Nếu chỉ thiếu Daypart, điền bằng mode (giá trị phổ biến nhất)\n",
    "most_common_daypart = df[\"Daypart\"].mode()[0]\n",
    "df[\"Daypart\"] = df[\"Daypart\"].fillna(most_common_daypart)\n",
    "\n",
    "# 🔸 Điền thiếu Visit#Daypart bằng median theo từng nhóm Daypart\n",
    "df[\"Visit#Daypart\"] = df.groupby(\"Daypart\")[\"Visit#Daypart\"].transform(lambda x: x.fillna(x.median()))\n",
    "\n",
    "# 🔸 Nếu còn giá trị thiếu do Daypart là null trước đó → fallback bằng median toàn bộ\n",
    "df[\"Visit#Daypart\"] = df[\"Visit#Daypart\"].fillna(df[\"Visit#Daypart\"].median())\n",
    "\n",
    "# 🔸 Ghi lại file sau xử lý\n",
    "df.to_csv(file_path, index=False)\n",
    "print(f\"✅ Đã xử lý và lưu lại tại: {file_path}\")\n",
    "# ──────────────────────────────────────────────\n",
    "# 🔹 Bước 5: Làm sạch Brand_Health_cleaned.csv – Spontaneous & Awareness\n",
    "# ──────────────────────────────────────────────\n",
    "file_path = \"./cleaned_data/Brand_Health_cleaned.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "print(\"\\n📁 File: Brand_Health_cleaned.csv\")\n",
    "print(f\"📏 Tổng số dòng: {df.shape[0]}\")\n",
    "\n",
    "# 🧭 Thống kê missing\n",
    "spontaneous_null = df[\"Spontaneous\"].isnull().sum()\n",
    "awareness_null = df[\"Awareness\"].isnull().sum()\n",
    "print(f\"🧭 Spontaneous missing: {spontaneous_null} ({round(100 * spontaneous_null / df.shape[0], 2)}%)\")\n",
    "print(f\"🧭 Awareness missing: {awareness_null} ({round(100 * awareness_null / df.shape[0], 2)}%)\")\n",
    "\n",
    "# 🔸 Gán cẩn thận: nếu Awareness bị thiếu nhưng Spontaneous có giá trị → gán Awareness = Spontaneous\n",
    "df.loc[df[\"Awareness\"].isnull() & df[\"Spontaneous\"].notnull(), \"Awareness\"] = df[\"Spontaneous\"]\n",
    "\n",
    "# 🔸 Sau khi suy luận, nếu vẫn thiếu Awareness → gán là \"Unaware\"\n",
    "df[\"Awareness\"] = df[\"Awareness\"].fillna(\"Unaware\")\n",
    "\n",
    "# 🔸 Ghi log cập nhật lại số null sau xử lý\n",
    "remaining_awareness_null = df[\"Awareness\"].isnull().sum()\n",
    "print(f\"✅ Awareness sau xử lý: còn {remaining_awareness_null} dòng thiếu.\")\n",
    "\n",
    "# 🔸 Sau khi suy luận, nếu vẫn thiếu Awareness → gán là \"Unaware\"\n",
    "df[\"Spontaneous\"] = df[\"Spontaneous\"].fillna(\"Unaware\")\n",
    "\n",
    "# 🔸 Ghi lại file\n",
    "df.to_csv(file_path, index=False)\n",
    "print(f\"✅ Đã xử lý và lưu lại tại: {file_path}\")\n",
    "import pandas as pd\n",
    "\n",
    "# Đọc file và làm sạch tên cột\n",
    "df = pd.read_csv(\"./cleaned_data/Brand_Health_cleaned.csv\", low_memory=False)\n",
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "# Bước 1: Chuyển các cột thành nhị phân (1 nếu có giá trị, 0 nếu NaN)\n",
    "df[\"Trial\"] = df[\"Trial\"].notnull().astype(int)\n",
    "df[\"P3M\"] = df[\"P3M\"].notnull().astype(int)\n",
    "df[\"P1M\"] = df[\"P1M\"].notnull().astype(int)\n",
    "\n",
    "# Bước 2: Áp dụng logic:\n",
    "# Nếu P3M = 1 → chắc chắn Trial = 1\n",
    "df.loc[(df[\"P3M\"] == 1) & (df[\"Trial\"] == 0), \"Trial\"] = 1\n",
    "\n",
    "# Nếu P1M = 1 → chắc chắn Trial = 1 và P3M = 1\n",
    "df.loc[df[\"P1M\"] == 1, \"Trial\"] = 1\n",
    "df.loc[(df[\"P1M\"] == 1) & (df[\"P3M\"] == 0), \"P3M\"] = 1\n",
    "\n",
    "# Thống kê sau xử lý\n",
    "print(\"✅ Sau khi ghi đè theo logic:\")\n",
    "print(f\"Trial = 1: {(df['Trial'] == 1).sum()} / {len(df)}\")\n",
    "print(f\"P3M   = 1: {(df['P3M'] == 1).sum()} / {len(df)}\")\n",
    "print(f\"P1M   = 1: {(df['P1M'] == 1).sum()} / {len(df)}\")\n",
    "\n",
    "# Ghi đè lại file gốc\n",
    "df.to_csv(\"./cleaned_data/Brand_Health_cleaned.csv\", index=False)\n",
    "print(\"💾 Đã ghi đè file: Brand_Health_cleaned.csv\")\n",
    "\n",
    "\n",
    "# Giả sử bạn đã load bảng vào df\n",
    "# Load dữ liệu\n",
    "df = pd.read_csv(\"cleaned_data/Brand_Health_cleaned.csv\")\n",
    "\n",
    "# 🔧 Chuẩn hóa tên cột\n",
    "df.columns = df.columns.str.strip().str.lower().str.replace('#', '_')\n",
    "\n",
    "# 🔹 1. Nếu fre_visit và spending đều có → tính lại PPA\n",
    "mask1 = df[\"fre_visit\"].notnull() & df[\"spending\"].notnull()\n",
    "df.loc[mask1, \"ppa\"] = df.loc[mask1, \"spending\"] / df.loc[mask1, \"fre_visit\"]\n",
    "\n",
    "# 🔹 2. Nếu fre_visit là NA → gán cả 4 cột = 0\n",
    "mask2 = df[\"fre_visit\"].isnull()\n",
    "df.loc[mask2, [\"fre_visit\", \"ppa\", \"spending\", \"spending_use\"]] = 0\n",
    "\n",
    "# 🔹 3. Nếu có 2/3 → tính phần còn lại\n",
    "# fre_visit + ppa → spending\n",
    "mask3 = df[\"fre_visit\"].notnull() & df[\"ppa\"].notnull() & df[\"spending\"].isnull()\n",
    "df.loc[mask3, \"spending\"] = df.loc[mask3, \"fre_visit\"] * df.loc[mask3, \"ppa\"]\n",
    "\n",
    "# fre_visit + spending → ppa\n",
    "mask4 = df[\"fre_visit\"].notnull() & df[\"spending\"].notnull() & df[\"ppa\"].isnull()\n",
    "df.loc[mask4, \"ppa\"] = df.loc[mask4, \"spending\"] / df.loc[mask4, \"fre_visit\"]\n",
    "\n",
    "# ppa + spending → fre_visit\n",
    "mask5 = df[\"ppa\"].notnull() & df[\"spending\"].notnull() & df[\"fre_visit\"].isnull()\n",
    "df.loc[mask5, \"fre_visit\"] = df.loc[mask5, \"spending\"] / df.loc[mask5, \"ppa\"]\n",
    "\n",
    "# 🔹 4. Nếu spending_use là NA → gán bằng spending\n",
    "df[\"spending_use\"] = df[\"spending_use\"].fillna(df[\"spending\"])\n",
    "# Gán ppa còn thiếu bằng median toàn bộ sample\n",
    "ppa_median = df[\"ppa\"].median()\n",
    "df[\"ppa\"] = df[\"ppa\"].fillna(ppa_median)\n",
    "\n",
    "# Tính lại spending = ppa * fre_visit nếu spending còn thiếu\n",
    "mask_spending_missing = df[\"spending\"].isnull()\n",
    "df.loc[mask_spending_missing, \"spending\"] = df.loc[mask_spending_missing, \"ppa\"] * df.loc[mask_spending_missing, \"fre_visit\"]\n",
    "\n",
    "# Gán spending_use bằng spending nếu còn thiếu\n",
    "df[\"spending_use\"] = df[\"spending_use\"].fillna(df[\"spending\"])\n",
    "\n",
    "# ✅ Lưu lại\n",
    "df.to_csv(\"cleaned_data/Brand_Health_cleaned.csv\", index=False)\n",
    "print(\"✅ Đã xử lý thành công!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a494f291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Kiểm tra các logic violations...\n",
      "\n",
      "❗ Vi phạm 1: P1M = 1 nhưng P3M != 1: 0 dòng\n",
      "❗ Vi phạm 2: P3M = 1 nhưng Trial != 1: 0 dòng\n",
      "❗ Vi phạm 3: Spending > 0 nhưng Fre_Visit thiếu hoặc bằng 0: 0 dòng\n",
      "❗ Vi phạm 4: Fre_Visit > 0 nhưng PPA thiếu: 0 dòng\n",
      "❗ Vi phạm 5: Fre_Visit > 0 nhưng Spending thiếu: 0 dòng\n",
      "❗ Vi phạm 6: Spending > 0 nhưng PPA thiếu: 0 dòng\n",
      "❗ Vi phạm 7: Có NPS nhưng Trial hoặc P3M = 0: 0 dòng\n",
      "❗ Vi phạm 8: Có Comprehension/Likability nhưng Awareness = 'Unaware': 4 dòng\n",
      "❗ Vi phạm 9: Có Spontaneous nhưng Awareness = 'Unaware': 90 dòng\n",
      "❗ Vi phạm 10: Spending_use bị thiếu: 0 dòng\n",
      "✅ Đã sửa 4 dòng vi phạm 8 (comprehension/likability → nhưng awareness = 'Unaware')\n",
      "✅ Đã sửa 82 dòng vi phạm 9 (spontaneous có nhưng awareness = 'Unaware')\n",
      "💾 Đã ghi đè file: Brand_Health_cleaned.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv(\"cleaned_data/Brand_Health_cleaned.csv\", low_memory=False)\n",
    "df.columns = df.columns.str.strip().str.lower().str.replace('#', '_')\n",
    "\n",
    "print(\"🔍 Kiểm tra các logic violations...\\n\")\n",
    "\n",
    "# 1. P1M = 1 nhưng P3M != 1\n",
    "violation_1 = df[(df[\"p1m\"] == 1) & (df[\"p3m\"] != 1)]\n",
    "print(f\"❗ Vi phạm 1: P1M = 1 nhưng P3M != 1: {len(violation_1)} dòng\")\n",
    "\n",
    "# 2. P3M = 1 nhưng Trial != 1\n",
    "violation_2 = df[(df[\"p3m\"] == 1) & (df[\"trial\"] != 1)]\n",
    "print(f\"❗ Vi phạm 2: P3M = 1 nhưng Trial != 1: {len(violation_2)} dòng\")\n",
    "\n",
    "# 3. Spending > 0 nhưng Fre_Visit = 0 hoặc NA\n",
    "violation_3 = df[(df[\"spending\"] > 0) & ((df[\"fre_visit\"].isnull()) | (df[\"fre_visit\"] == 0))]\n",
    "print(f\"❗ Vi phạm 3: Spending > 0 nhưng Fre_Visit thiếu hoặc bằng 0: {len(violation_3)} dòng\")\n",
    "\n",
    "# 4. Fre_Visit > 0 nhưng PPA thiếu\n",
    "violation_4 = df[(df[\"fre_visit\"] > 0) & (df[\"ppa\"].isnull())]\n",
    "print(f\"❗ Vi phạm 4: Fre_Visit > 0 nhưng PPA thiếu: {len(violation_4)} dòng\")\n",
    "\n",
    "# 5. Fre_Visit > 0 nhưng Spending thiếu\n",
    "violation_5 = df[(df[\"fre_visit\"] > 0) & (df[\"spending\"].isnull())]\n",
    "print(f\"❗ Vi phạm 5: Fre_Visit > 0 nhưng Spending thiếu: {len(violation_5)} dòng\")\n",
    "\n",
    "# 6. Spending > 0 nhưng PPA thiếu\n",
    "violation_6 = df[(df[\"spending\"] > 0) & (df[\"ppa\"].isnull())]\n",
    "print(f\"❗ Vi phạm 6: Spending > 0 nhưng PPA thiếu: {len(violation_6)} dòng\")\n",
    "\n",
    "# 7. NPS có nhưng Trial hoặc P3M = 0\n",
    "violation_7 = df[(df[\"nps_p3m\"].notnull()) & ((df[\"trial\"] != 1) | (df[\"p3m\"] != 1))]\n",
    "print(f\"❗ Vi phạm 7: Có NPS nhưng Trial hoặc P3M = 0: {len(violation_7)} dòng\")\n",
    "\n",
    "# 8. Có Comprehension hoặc Brand_Likability nhưng Awareness = 'Unaware'\n",
    "violation_8 = df[\n",
    "    ((df[\"comprehension\"].notnull()) | (df[\"brand_likability\"].notnull())) &\n",
    "    (df[\"awareness\"].str.lower() == \"unaware\")\n",
    "]\n",
    "print(f\"❗ Vi phạm 8: Có Comprehension/Likability nhưng Awareness = 'Unaware': {len(violation_8)} dòng\")\n",
    "\n",
    "# 9. Spontaneous có nhưng Awareness = 'Unaware'\n",
    "violation_9 = df[(df[\"spontaneous\"].notnull()) & (df[\"awareness\"].str.lower() == \"unaware\")]\n",
    "print(f\"❗ Vi phạm 9: Có Spontaneous nhưng Awareness = 'Unaware': {len(violation_9)} dòng\")\n",
    "\n",
    "# 10. Spending_use bị thiếu\n",
    "violation_10 = df[df[\"spending_use\"].isnull()]\n",
    "print(f\"❗ Vi phạm 10: Spending_use bị thiếu: {len(violation_10)} dòng\")\n",
    "import pandas as pd\n",
    "\n",
    "# Đọc dữ liệu\n",
    "df = pd.read_csv(\"cleaned_data/Brand_Health_cleaned.csv\", low_memory=False)\n",
    "\n",
    "# Chuẩn hóa tên cột\n",
    "df.columns = df.columns.str.strip().str.lower().str.replace('#', '_')\n",
    "\n",
    "# Tạo cờ ban đầu nếu chưa có\n",
    "if \"was_awareness_corrected\" not in df.columns:\n",
    "    df[\"was_awareness_corrected\"] = 0\n",
    "\n",
    "# ─────────────────────────────────────────────\n",
    "# 🔹 Vi phạm 8: Có comprehension/likability nhưng awareness = \"Unaware\"\n",
    "mask8 = (\n",
    "    ((df[\"comprehension\"].notnull()) | (df[\"brand_likability\"].notnull())) &\n",
    "    (df[\"awareness\"].str.lower() == \"unaware\")\n",
    ")\n",
    "\n",
    "# Gán lại thành \"Aware\"\n",
    "df.loc[mask8, \"awareness\"] = \"Aware\"\n",
    "df.loc[mask8, \"was_awareness_corrected\"] = 1\n",
    "print(f\"✅ Đã sửa {mask8.sum()} dòng vi phạm 8 (comprehension/likability → nhưng awareness = 'Unaware')\")\n",
    "\n",
    "# ─────────────────────────────────────────────\n",
    "# 🔹 Vi phạm 9: Có spontaneous nhưng awareness = \"Unaware\"\n",
    "mask9 = (df[\"spontaneous\"].notnull()) & (df[\"awareness\"].str.lower() == \"unaware\")\n",
    "\n",
    "# Gán lại awareness = spontaneous\n",
    "df.loc[mask9, \"awareness\"] = df.loc[mask9, \"spontaneous\"]\n",
    "df.loc[mask9, \"was_awareness_corrected\"] = 1\n",
    "print(f\"✅ Đã sửa {mask9.sum()-4} dòng vi phạm 9 (spontaneous có nhưng awareness = 'Unaware')\")\n",
    "\n",
    "# Ghi lại kết quả\n",
    "df.to_csv(\"cleaned_data/Brand_Health_cleaned.csv\", index=False)\n",
    "print(\"💾 Đã ghi đè file: Brand_Health_cleaned.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cdc5e505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❗ Có 0 dòng có Needstates nhưng thiếu NeedstateGroup\n",
      "\n",
      "🔍 Kiểm tra logic bảng: Competitor Database\n",
      "❗ Dòng có storecount < 0 hoặc thiếu: 0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Đọc file\n",
    "df = pd.read_csv(\"cleaned_data/NeedstateDayDaypart_cleaned.csv\")\n",
    "df.columns = df.columns.str.strip().str.lower().str.replace('#', '_')\n",
    "\n",
    "# 2️⃣ Needstates có nhưng thiếu needstategroup\n",
    "needstate_logic_mismatch = df[(df[\"needstates\"].notnull()) & (df[\"needstategroup\"].isnull())]\n",
    "print(f\"❗ Có {len(needstate_logic_mismatch)} dòng có Needstates nhưng thiếu NeedstateGroup\")\n",
    "\n",
    "\n",
    "# Đọc dữ liệu\n",
    "df = pd.read_csv(\"cleaned_data/Competitor_cleaned.csv\")\n",
    "df.columns = df.columns.str.strip().str.lower().str.replace('#', '_')\n",
    "\n",
    "print(\"\\n🔍 Kiểm tra logic bảng: Competitor Database\")\n",
    "\n",
    "# 1️⃣ storecount phải là số nguyên và không âm\n",
    "storecount_invalid = df[(df[\"storecount\"] < 0) | (df[\"storecount\"].isnull())]\n",
    "print(f\"❗ Dòng có storecount < 0 hoặc thiếu: {len(storecount_invalid)}\")\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Đọc dữ liệu\n",
    "dayofweek = pd.read_csv(\"cleaned_data/Dayofweek_cleaned.csv\")\n",
    "daypart = pd.read_csv(\"cleaned_data/Daypart_cleaned.csv\")\n",
    "\n",
    "# Chuẩn hóa tên cột\n",
    "dayofweek.columns = dayofweek.columns.str.strip().str.lower().str.replace('#', '_')\n",
    "daypart.columns = daypart.columns.str.strip().str.lower().str.replace('#', '_')\n",
    "\n",
    "# ----------- Kiểm tra Dayofweek.csv -----------\n",
    "\n",
    "# 1️⃣ Kiểm tra dayofweek có đúng tên thứ không\n",
    "valid_days = {\"monday\", \"tuesday\", \"wednesday\", \"thursday\", \"friday\", \"saturday\", \"sunday\"}\n",
    "invalid_day_names = dayofweek[~dayofweek[\"dayofweek\"].str.lower().isin(valid_days)]\n",
    "\n",
    "# 2️⃣ Kiểm tra visit_dayofweek có giá trị không âm và không null\n",
    "invalid_visits = dayofweek[(dayofweek[\"visit_dayofweek\"].isnull()) | (dayofweek[\"visit_dayofweek\"] < 0)]\n",
    "\n",
    "# 3️⃣ Kiểm tra weakday_end có khớp với dayofweek\n",
    "def map_weekgroup(day):\n",
    "    return \"Weekend\" if day.lower() in [\"saturday\", \"sunday\"] else \"Weekdays\"\n",
    "\n",
    "dayofweek[\"expected_group\"] = dayofweek[\"dayofweek\"].str.lower().apply(map_weekgroup)\n",
    "wrong_group = dayofweek[dayofweek[\"weekday_end\"].str.lower() != dayofweek[\"expected_group\"].str.lower()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9b5c4692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Group_size không hợp lệ:\n",
      "Empty DataFrame\n",
      "Columns: [id, group_size]\n",
      "Index: [] \n",
      "\n",
      "❌ Age group không khớp với Age:\n",
      "Empty DataFrame\n",
      "Columns: [id, age, age_group_2, expected_age_group]\n",
      "Index: [] \n",
      "\n",
      "❌ MPI group không khớp với MPI_Mean:\n",
      "Empty DataFrame\n",
      "Columns: [id, mpi_mean, mpi_2, expected_mpi_2]\n",
      "Index: [] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Đọc dữ liệu\n",
    "df = pd.read_csv(\"cleaned_data/SA_var_cleaned.csv\")\n",
    "df.columns = df.columns.str.strip().str.lower().str.replace('#', '_')\n",
    "\n",
    "# 🔹 Group_size không hợp lệ\n",
    "invalid_group_size = df[(df[\"group_size\"].isnull()) | (df[\"group_size\"] < 1)]\n",
    "print(\"❌ Group_size không hợp lệ:\")\n",
    "print(invalid_group_size[[\"id\", \"group_size\"]].head(15), \"\\n\")\n",
    "\n",
    "# 🔹 Age group không khớp với tuổi\n",
    "def map_age_group(age):\n",
    "    if age <= 19: return '16 - 19 y.o.'\n",
    "    elif age <= 24: return '20 - 24 y.o.'\n",
    "    elif age <= 29: return '25 - 29 y.o.'\n",
    "    elif age <= 34: return '30 - 34 y.o.'\n",
    "    elif age <= 39: return '35 - 39 y.o.'\n",
    "    elif age <= 44: return '40 - 44 y.o.'\n",
    "    else: return '45+ y.o.'\n",
    "\n",
    "df[\"expected_age_group\"] = df[\"age\"].apply(map_age_group)\n",
    "age_group_mismatch = df[df[\"age_group_2\"] != df[\"expected_age_group\"]]\n",
    "print(\"❌ Age group không khớp với Age:\")\n",
    "print(age_group_mismatch[[\"id\", \"age\", \"age_group_2\", \"expected_age_group\"]].head(10), \"\\n\")\n",
    "\n",
    "# 🔹 MPI group không khớp với MPI_mean\n",
    "def map_mpi_group(mpi):\n",
    "    if mpi < 4500: return \"1.Under VND 4.5m\"\n",
    "    elif mpi < 9000: return \"2.VND 4.5m - VND 8.9m\"\n",
    "    elif mpi < 15000: return \"3.VND 9m - VND 14.9m\"\n",
    "    elif mpi < 25000: return \"4.VND 15m - VND 24.9m\"\n",
    "    else: return \"5.VND 25m+\"\n",
    "\n",
    "df[\"expected_mpi_2\"] = df[\"mpi_mean\"].apply(map_mpi_group)\n",
    "mpi_group_mismatch = df[df[\"mpi_2\"] != df[\"expected_mpi_2\"]]\n",
    "print(\"❌ MPI group không khớp với MPI_Mean:\")\n",
    "print(mpi_group_mismatch[[\"id\", \"mpi_mean\", \"mpi_2\", \"expected_mpi_2\"]].head(10), \"\\n\")\n",
    "\n",
    "# 🔹 TOM, BUMO, MostFavourite đều trùng nhau\n",
    "same_brand_all = df[\n",
    "    (df[\"tom\"].notnull()) & \n",
    "    (df[\"bumo\"].notnull()) & \n",
    "    (df[\"mostfavourite\"].notnull()) & \n",
    "    (df[\"tom\"] == df[\"bumo\"]) & \n",
    "    (df[\"bumo\"] == df[\"mostfavourite\"])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ccd2d6e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\phamh\\AppData\\Local\\Temp\\ipykernel_22736\\1711239559.py:28: DtypeWarning: Columns (8,9,10,14,16) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Tất cả các ID đều có mặt trong SA_var_cleaned.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Đường dẫn thư mục chứa file\n",
    "data_dir = \"cleaned_data/\"\n",
    "\n",
    "# Đọc ID từ bảng gốc (bảng chính)\n",
    "sa = pd.read_csv(os.path.join(data_dir, \"SA_var_cleaned.csv\"))\n",
    "sa.columns = sa.columns.str.strip().str.lower()\n",
    "valid_ids = set(sa[\"id\"].dropna().unique())\n",
    "\n",
    "# Danh sách các file cần kiểm tra ID\n",
    "files_to_check = [\n",
    "    \"Brand_Health_cleaned.csv\",\n",
    "    \"Brand_Image_cleaned.csv\",\n",
    "    \"Companion_cleaned.csv\",\n",
    "    \"Dayofweek_cleaned.csv\",\n",
    "    \"Daypart_cleaned.csv\",\n",
    "    \"NeedstateDayDaypart_cleaned.csv\"\n",
    "]\n",
    "\n",
    "# Lưu kết quả lỗi\n",
    "missing_id_report = []\n",
    "\n",
    "# Lặp qua từng file và kiểm tra ID\n",
    "for file_name in files_to_check:\n",
    "    path = os.path.join(data_dir, file_name)\n",
    "    df = pd.read_csv(path)\n",
    "    df.columns = df.columns.str.strip().str.lower()\n",
    "    \n",
    "    if \"id\" in df.columns:\n",
    "        ids = set(df[\"id\"].dropna().unique())\n",
    "        missing_ids = ids - valid_ids\n",
    "        if missing_ids:\n",
    "            missing_id_report.append({\n",
    "                \"file\": file_name,\n",
    "                \"missing_count\": len(missing_ids),\n",
    "                \"sample_missing_ids\": list(missing_ids)[:5]  # in mẫu 5 ID\n",
    "            })\n",
    "\n",
    "# In kết quả\n",
    "if missing_id_report:\n",
    "    print(\"❌ Một số file có ID không tồn tại trong SA_var_cleaned.csv:\")\n",
    "    for report in missing_id_report:\n",
    "        print(f\"📁 {report['file']}: thiếu {report['missing_count']} ID\")\n",
    "        print(f\"   → Ví dụ ID thiếu: {report['sample_missing_ids']}\")\n",
    "else:\n",
    "    print(\"✅ Tất cả các ID đều có mặt trong SA_var_cleaned.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a1d51f7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\phamh\\AppData\\Local\\Temp\\ipykernel_22736\\2020792781.py:13: DtypeWarning: Columns (8,9,10,14,16) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📁 File: Brand_Health_cleaned.csv\n",
      "➡️ Features: ['id', 'brand', 'spontaneous', 'awareness', 'trial', 'p3m', 'p1m', 'comprehension', 'brand_likability', 'weekly', 'daily', 'fre_visit', 'ppa', 'spending', 'segmentation', 'nps_p3m', 'nps_p3m_group', 'spending_use', 'was_awareness_corrected']\n",
      "--------------------------------------------------------------------------------\n",
      "📁 File: Brand_Image_cleaned.csv\n",
      "➡️ Features: ['ID', 'Awareness', 'Attribute', 'BrandImage']\n",
      "--------------------------------------------------------------------------------\n",
      "📁 File: Companion_cleaned.csv\n",
      "➡️ Features: ['ID', 'Companion#group']\n",
      "--------------------------------------------------------------------------------\n",
      "📁 File: Competitor_cleaned.csv\n",
      "➡️ Features: ['No#', 'Brand', 'City', 'Year', 'StoreCount']\n",
      "--------------------------------------------------------------------------------\n",
      "📁 File: Dayofweek_cleaned.csv\n",
      "➡️ Features: ['ID', 'Dayofweek', 'Visit#Dayofweek', 'Weekday#end']\n",
      "--------------------------------------------------------------------------------\n",
      "📁 File: Daypart_cleaned.csv\n",
      "➡️ Features: ['ID', 'Daypart', 'Visit#Daypart']\n",
      "--------------------------------------------------------------------------------\n",
      "📁 File: NeedstateDayDaypart_cleaned.csv\n",
      "➡️ Features: ['ID', 'Needstates', 'Day#Daypart', 'NeedstateGroup']\n",
      "--------------------------------------------------------------------------------\n",
      "📁 File: SA_var_cleaned.csv\n",
      "➡️ Features: ['ID', 'City', 'Group_size', 'Age', 'MPI#Mean', 'TOM', 'BUMO', 'BUMO_Previous', 'MostFavourite', 'Gender', 'Age#Group#2', 'MPI#2', 'Occupation', 'Occupation#group', 'Year']\n",
      "--------------------------------------------------------------------------------\n",
      "📁 File: Segmentation_cleaned.csv\n",
      "➡️ Features: ['ID', 'Segmentation', 'Visit', 'Spending', 'Brand', 'PPA']\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Đường dẫn tới thư mục chứa các file đã làm sạch\n",
    "data_dir = \"cleaned_data\"\n",
    "\n",
    "# Duyệt tất cả các file trong thư mục\n",
    "for filename in os.listdir(data_dir):\n",
    "    # Chỉ xét các file CSV (hoặc đổi điều kiện nếu là định dạng khác)\n",
    "    if filename.endswith(\".csv\"):\n",
    "        file_path = os.path.join(data_dir, filename)\n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "            print(f\"📁 File: {filename}\")\n",
    "            print(\"➡️ Features:\", list(df.columns))\n",
    "            print(\"-\" * 80)\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Lỗi đọc file {filename}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "03444d5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Số bản ghi có spending nhưng thiếu fre_visit: 0\n",
      "Empty DataFrame\n",
      "Columns: [id, spending, fre_visit, ppa]\n",
      "Index: []\n",
      "\n",
      "Số ID năm 2017 nhưng không có trong Segmentation: 9\n",
      "\n",
      "== Thông tin trong Brand_Health của các ID bị thiếu trong Segmentation ==\n",
      "           id  fre_visit  spending  ppa\n",
      "4779   138658        0.0       0.0  0.0\n",
      "5873   140223        0.0       0.0  0.0\n",
      "5897   141272        0.0       0.0  0.0\n",
      "6130   141270        0.0       0.0  0.0\n",
      "6434   138841        0.0       0.0  0.0\n",
      "6455   140225        0.0       0.0  0.0\n",
      "6797   140345        0.0       0.0  0.0\n",
      "25909  138694        0.0       0.0  0.0\n",
      "26007  140253        0.0       0.0  0.0\n",
      "57134  140253       30.0       0.0  0.0\n",
      "58111  140225        4.0       0.0  0.0\n",
      "58125  141270        4.0       0.0  0.0\n",
      "58240  138841        8.0       0.0  0.0\n",
      "58269  140223        8.0       0.0  0.0\n",
      "58574  141272        4.0       0.0  0.0\n",
      "59323  138658        4.0       0.0  0.0\n",
      "59399  140345        4.0       0.0  0.0\n",
      "60197  138694        4.0       0.0  0.0\n",
      "\n",
      "== Kiểm tra lại các ID này trong Segmentation ==\n",
      "Empty DataFrame\n",
      "Columns: [id, segmentation, visit, spending, brand, ppa]\n",
      "Index: []\n",
      "       id  fre_visit  spending  ppa\n",
      "0  138658   0.800000       0.0  0.0\n",
      "1  138694   0.666667       0.0  0.0\n",
      "2  138841   1.333333       0.0  0.0\n",
      "3  140223   1.000000       0.0  0.0\n",
      "4  140225   0.571429       0.0  0.0\n",
      "5  140253   6.000000       0.0  0.0\n",
      "6  140345   0.666667       0.0  0.0\n",
      "7  141270   0.666667       0.0  0.0\n",
      "8  141272   0.363636       0.0  0.0\n",
      "\n",
      "✅ Đã cập nhật Segmentation_cleaned.csv với 9 ID không thể phân loại.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\phamh\\AppData\\Local\\Temp\\ipykernel_22736\\111309359.py:5: DtypeWarning: Columns (8,9,10,14,16) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_health = pd.read_csv(\"cleaned_data/Brand_Health_cleaned.csv\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Đọc dữ liệu\n",
    "df_seg = pd.read_csv(\"cleaned_data/Segmentation_cleaned.csv\")\n",
    "df_health = pd.read_csv(\"cleaned_data/Brand_Health_cleaned.csv\")\n",
    "df_sa = pd.read_csv(\"cleaned_data/SA_var_cleaned.csv\")\n",
    "\n",
    "# Chuẩn hóa tên cột về chữ thường để tránh lỗi\n",
    "df_health.columns = df_health.columns.str.lower()\n",
    "df_seg.columns = df_seg.columns.str.lower()\n",
    "df_sa.columns = df_sa.columns.str.lower()\n",
    "\n",
    "# 1. Kiểm tra bản ghi có spending không null nhưng fre_visit bị thiếu\n",
    "inconsistent_rows = df_health[df_health['spending'].notna() & df_health['fre_visit'].isna()]\n",
    "print(f\"Số bản ghi có spending nhưng thiếu fre_visit: {len(inconsistent_rows)}\")\n",
    "print(inconsistent_rows[['id', 'spending', 'fre_visit', 'ppa']].head(10))\n",
    "\n",
    "# 2. Lọc các ID trong SA_var có Year = 2017\n",
    "ids_2017 = df_sa[df_sa['year'] == 2017]['id'].unique()\n",
    "\n",
    "# 3. So sánh: tìm các ID năm 2017 không có trong Segmentation\n",
    "missing_ids = set(ids_2017) - set(df_seg['id'])\n",
    "print(f\"\\nSố ID năm 2017 nhưng không có trong Segmentation: {len(missing_ids)}\")\n",
    "missing_ids = list(missing_ids)\n",
    "\n",
    "# 4. Kiểm tra thông tin brand health của các ID bị thiếu\n",
    "missing_health = df_health[df_health['id'].isin(missing_ids)]\n",
    "print(\"\\n== Thông tin trong Brand_Health của các ID bị thiếu trong Segmentation ==\")\n",
    "print(missing_health[['id', 'fre_visit', 'spending', 'ppa']].drop_duplicates())\n",
    "\n",
    "# 5. Xác nhận lại các ID này có trong Segmentation không\n",
    "print(\"\\n== Kiểm tra lại các ID này trong Segmentation ==\")\n",
    "print(df_seg[df_seg['id'].isin(missing_ids)])\n",
    "\n",
    "# 6. Kiểm tra dữ liệu hành vi từ Brand_Health cho các ID bị thiếu\n",
    "# Lấy giá trị trung bình (hoặc tối đa, hoặc tổng tuỳ mục đích)\n",
    "summary = missing_health.groupby('id')[['fre_visit', 'spending', 'ppa']].mean().reset_index()\n",
    "print(summary)\n",
    "# 7. Tạo bản ghi mới cho các ID không thể phân loại\n",
    "# Gán segmentation = 'Not Segmentable', Brand = 0\n",
    "new_rows = pd.DataFrame({\n",
    "    'id': missing_ids,\n",
    "    'segmentation': 'Not Segmentable',\n",
    "    'visit': None,\n",
    "    'spending': None,\n",
    "    'brand': 0,\n",
    "    'ppa': None\n",
    "})\n",
    "\n",
    "# 8. Gộp lại với bảng segmentation gốc\n",
    "df_seg_updated = pd.concat([df_seg, new_rows], ignore_index=True)\n",
    "\n",
    "# 9. Xuất ra file mới nếu cần\n",
    "df_seg_updated.to_csv(\"cleaned_data/Segmentation_cleaned.csv\", index=False)\n",
    "print(\"\\n✅ Đã cập nhật Segmentation_cleaned.csv với 9 ID không thể phân loại.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "31235dd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Số dòng inconsistency A1: 0\n",
      "Empty DataFrame\n",
      "Columns: [id, brand, health_ppa, seg_ppa_calc, seg_ppa, abs_diff_ppa]\n",
      "Index: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\phamh\\AppData\\Local\\Temp\\ipykernel_22736\\3033574368.py:6: DtypeWarning: Columns (8,9,10,14,16) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_health = pd.read_csv(\"cleaned_data/Brand_Health_cleaned.csv\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load dữ liệu\n",
    "df_sa = pd.read_csv(\"cleaned_data/SA_var_cleaned.csv\")\n",
    "df_health = pd.read_csv(\"cleaned_data/Brand_Health_cleaned.csv\")\n",
    "df_seg = pd.read_csv(\"cleaned_data/Segmentation_cleaned.csv\")\n",
    "\n",
    "# Chuẩn hóa tên cột về chữ thường để tránh lỗi\n",
    "df_sa.columns = df_sa.columns.str.lower()\n",
    "df_health.columns = df_health.columns.str.lower()\n",
    "df_seg.columns = df_seg.columns.str.lower()\n",
    "\n",
    "# Chọn ID có Year = 2017\n",
    "id_2017 = df_sa[df_sa['year'] == 2017]['id'].unique()\n",
    "\n",
    "# Lọc dữ liệu liên quan\n",
    "df_health_2017 = df_health[df_health['id'].isin(id_2017)].copy()\n",
    "df_seg_2017 = df_seg[df_seg['id'].isin(id_2017)].copy()\n",
    "\n",
    "# Đổi tên để merge và so sánh\n",
    "df_health_2017 = df_health_2017.rename(columns={\n",
    "    'ppa': 'health_ppa',\n",
    "    'spending': 'health_spending',\n",
    "    'fre_visit': 'health_visit'\n",
    "})\n",
    "\n",
    "df_seg_2017 = df_seg_2017.rename(columns={\n",
    "    'ppa': 'seg_ppa',\n",
    "    'spending': 'seg_spending',\n",
    "    'visit': 'seg_visit'\n",
    "})\n",
    "\n",
    "# Merge theo ID và Brand\n",
    "merged = pd.merge(df_health_2017, df_seg_2017, on=['id', 'brand'], how='inner')\n",
    "\n",
    "# Ép kiểu số\n",
    "for col in ['health_ppa', 'health_spending', 'health_visit', 'seg_ppa', 'seg_spending', 'seg_visit']:\n",
    "    merged[col] = pd.to_numeric(merged[col], errors='coerce')\n",
    "\n",
    "# Tính lại PPA\n",
    "merged['seg_ppa_calc'] = merged['seg_spending'] / merged['seg_visit']\n",
    "\n",
    "# Tính lệch tuyệt đối và lệch tương đối\n",
    "merged['abs_diff_ppa'] = np.abs(merged['health_ppa'] - merged['seg_ppa_calc'])\n",
    "\n",
    "condition = (\n",
    "    (merged['abs_diff_ppa'] > 1) |\n",
    "    ((merged['abs_diff_ppa'] / merged['health_ppa'].replace(0, np.nan)) > 0.2)\n",
    ")\n",
    "\n",
    "# Lọc inconsistency\n",
    "inconsistencies = merged[condition]\n",
    "\n",
    "print(f\"Số dòng inconsistency A1: {len(inconsistencies)}\")\n",
    "print(inconsistencies[['id', 'brand', 'health_ppa', 'seg_ppa_calc', 'seg_ppa', 'abs_diff_ppa']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "227f6018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C1 – Số dòng segmentation không khớp: 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>segmentation_health</th>\n",
       "      <th>segmentation_seg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [id, segmentation_health, segmentation_seg]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_health.columns = df_health.columns.str.lower()\n",
    "df_sa.columns = df_sa.columns.str.lower()\n",
    "df_seg.columns = df_seg.columns.str.lower()\n",
    "\n",
    "# Bây giờ cột 'ID' trở thành 'id', 'Segmentation' thành 'segmentation'\n",
    "df_merge_c1 = df_health[['id', 'segmentation']].merge(\n",
    "    df_sa[['id']], on='id', how='inner'\n",
    ").merge(\n",
    "    df_seg[['id', 'segmentation']], on='id', how='left', suffixes=('_health', '_seg')\n",
    ")\n",
    "\n",
    "# So sánh segmentation giữa các bảng\n",
    "df_c1_inconsistent = df_merge_c1[\n",
    "    (df_merge_c1['segmentation_health'].notna()) &\n",
    "    (df_merge_c1['segmentation_seg'].notna()) &\n",
    "    (df_merge_c1['segmentation_health'] != df_merge_c1['segmentation_seg'])\n",
    "]\n",
    "\n",
    "print(f\"C1 – Số dòng segmentation không khớp: {len(df_c1_inconsistent)}\")\n",
    "display(df_c1_inconsistent[['id', 'segmentation_health', 'segmentation_seg']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b7c81410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C1 – Số dòng segmentation không khớp: 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>segmentation_health</th>\n",
       "      <th>segmentation_seg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [id, segmentation_health, segmentation_seg]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C4 – Số dòng có group_size < 1 hoặc thiếu: 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>group_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [id, group_size]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C5 – Số dòng spending_use > spending: 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>spending</th>\n",
       "      <th>spending_use</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [id, spending, spending_use]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C3 – Số dòng có brand không hợp lệ: 42050\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>brand</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>349551</td>\n",
       "      <td>milano</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>349553</td>\n",
       "      <td>milano</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365</th>\n",
       "      <td>349957</td>\n",
       "      <td>milano</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>366</th>\n",
       "      <td>350148</td>\n",
       "      <td>milano</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>367</th>\n",
       "      <td>351103</td>\n",
       "      <td>milano</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74413</th>\n",
       "      <td>456854</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74414</th>\n",
       "      <td>456857</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74415</th>\n",
       "      <td>458063</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74416</th>\n",
       "      <td>458098</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74418</th>\n",
       "      <td>459890</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>38383 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id   brand\n",
       "363    349551  milano\n",
       "364    349553  milano\n",
       "365    349957  milano\n",
       "366    350148  milano\n",
       "367    351103  milano\n",
       "...       ...     ...\n",
       "74413  456854   other\n",
       "74414  456857   other\n",
       "74415  458063   other\n",
       "74416  458098   other\n",
       "74418  459890   other\n",
       "\n",
       "[38383 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Các brand không hợp lệ trong Brand_Health:\n",
      "brand\n",
      "street                    10669\n",
      "other                      9733\n",
      "milano                     4699\n",
      "independent cafe           2717\n",
      "aha cafe                   1852\n",
      "urban station              1667\n",
      "passio                     1627\n",
      "thức coffee                1061\n",
      "viva star                  1029\n",
      "mê trang                    836\n",
      "coffee bean & tea leaf      824\n",
      "long cafe                   710\n",
      "gong cha                    594\n",
      "mộc miên                    585\n",
      "đen đá                      520\n",
      "effoc                       473\n",
      "maxx coffee                 397\n",
      "saigon café                 363\n",
      "bonpas                      316\n",
      "runam cafe                  316\n",
      "nia cafe                    263\n",
      "the coffee factory          256\n",
      "koi cafe                    237\n",
      "the cups coffee             133\n",
      "cheese coffee               123\n",
      "laha coffee                  50\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load tất cả datasets\n",
    "df_health = pd.read_csv(\"cleaned_data/Brand_Health_cleaned.csv\", dtype=str, low_memory=False)\n",
    "df_sa = pd.read_csv(\"cleaned_data/SA_var_cleaned.csv\", dtype=str)\n",
    "df_seg = pd.read_csv(\"cleaned_data/Segmentation_cleaned.csv\", dtype=str)\n",
    "df_dow = pd.read_csv(\"cleaned_data/Dayofweek_cleaned.csv\", dtype=str)\n",
    "df_dp = pd.read_csv(\"cleaned_data/Daypart_cleaned.csv\", dtype=str)\n",
    "df_comp = pd.read_csv(\"cleaned_data/Competitor_cleaned.csv\", dtype=str)\n",
    "\n",
    "# Chuẩn hóa tên cột thành chữ thường để tránh lỗi về KeyError\n",
    "df_health.columns = df_health.columns.str.lower()\n",
    "df_sa.columns = df_sa.columns.str.lower()\n",
    "df_seg.columns = df_seg.columns.str.lower()\n",
    "df_dow.columns = df_dow.columns.str.lower()\n",
    "df_dp.columns = df_dp.columns.str.lower()\n",
    "df_comp.columns = df_comp.columns.str.lower()\n",
    "\n",
    "# Chuyển các cột cần thiết sang kiểu số\n",
    "df_health['fre_visit'] = pd.to_numeric(df_health['fre_visit'], errors='coerce')\n",
    "df_health['spending'] = pd.to_numeric(df_health['spending'], errors='coerce')\n",
    "df_health['spending_use'] = pd.to_numeric(df_health['spending_use'], errors='coerce')\n",
    "df_dow['visit#dayofweek'] = pd.to_numeric(df_dow['visit#dayofweek'], errors='coerce')\n",
    "df_dp['visit#daypart'] = pd.to_numeric(df_dp['visit#daypart'], errors='coerce')\n",
    "df_sa['group_size'] = pd.to_numeric(df_sa['group_size'], errors='coerce')\n",
    "\n",
    "# =============================\n",
    "# ✅ C1. Kiểm tra sự không khớp của segmentation giữa các bảng\n",
    "# =============================\n",
    "# Ghép df_health (chỉ lấy id, segmentation) với df_sa (chỉ lấy id) rồi merge với df_seg (id, segmentation)\n",
    "df_merge_c1 = df_health[['id', 'segmentation']].merge(\n",
    "    df_sa[['id']], on='id', how='inner'\n",
    ").merge(\n",
    "    df_seg[['id', 'segmentation']], on='id', how='left', suffixes=('_health', '_seg')\n",
    ")\n",
    "\n",
    "# Lọc các dòng mà segmentation khác nhau (với cả 2 bảng đều khác NaN)\n",
    "df_c1_inconsistent = df_merge_c1[\n",
    "    (df_merge_c1['segmentation_health'].notna()) &\n",
    "    (df_merge_c1['segmentation_seg'].notna()) &\n",
    "    (df_merge_c1['segmentation_health'] != df_merge_c1['segmentation_seg'])\n",
    "]\n",
    "\n",
    "print(f\"C1 – Số dòng segmentation không khớp: {len(df_c1_inconsistent)}\")\n",
    "display(df_c1_inconsistent[['id', 'segmentation_health', 'segmentation_seg']])\n",
    "\n",
    "# =============================\n",
    "# ✅ C4. Kiểm tra group_size phải >= 1\n",
    "# =============================\n",
    "df_c4_invalid = df_sa[(df_sa['group_size'] < 1) | (df_sa['group_size'].isna())]\n",
    "print(f\"C4 – Số dòng có group_size < 1 hoặc thiếu: {len(df_c4_invalid)}\")\n",
    "display(df_c4_invalid[['id', 'group_size']])\n",
    "\n",
    "# =============================\n",
    "# ✅ C5. Kiểm tra spending_use ≤ spending\n",
    "# =============================\n",
    "df_c5_invalid = df_health[\n",
    "    (df_health['spending'].notna()) &\n",
    "    (df_health['spending_use'].notna()) &\n",
    "    (df_health['spending_use'] > df_health['spending'])\n",
    "]\n",
    "print(f\"C5 – Số dòng spending_use > spending: {len(df_c5_invalid)}\")\n",
    "display(df_c5_invalid[['id', 'spending', 'spending_use']])\n",
    "\n",
    "# =============================\n",
    "# ✅ C3. Kiểm tra brand phải nằm trong danh sách brand hợp lệ (danh sách lấy từ Competitor)\n",
    "# =============================\n",
    "# Chuẩn hóa cột brand cho cả df_health và df_comp\n",
    "df_health['brand'] = df_health['brand'].str.lower().str.strip()\n",
    "df_comp['brand'] = df_comp['brand'].str.lower().str.strip()\n",
    "\n",
    "# Lấy danh sách các brand hợp lệ\n",
    "valid_brands = df_comp['brand'].dropna().unique()\n",
    "\n",
    "# Lọc các dòng trong Brand_Health có brand không thuộc danh sách hợp lệ\n",
    "df_c3_invalid = df_health[~df_health['brand'].isin(valid_brands)]\n",
    "print(f\"C3 – Số dòng có brand không hợp lệ: {len(df_c3_invalid)}\")\n",
    "display(df_c3_invalid[['id', 'brand']].drop_duplicates())\n",
    "print(\"Các brand không hợp lệ trong Brand_Health:\")\n",
    "print(df_c3_invalid['brand'].value_counts())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
